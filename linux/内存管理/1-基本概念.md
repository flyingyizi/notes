
以下内容涉及linux source版本为“4.14.21”，如果不特殊说明，都是以arm32 arch相关的内容。


# 基本概念

基本缩写:linux\Documentation\arm\Porting

## 硬件基础

[TTBR0/TTBR1](https://www.cnblogs.com/DF11G/p/14486558.html)，它的具体实现可以参阅<ARM Architecture Reference Manual（ARMv7-AR)> Page1709:

ARMv7-A根据TTBCR寄存器的N位决定使用TTBR0还是TTBR1. 

  - 如果TTBCR.N = 0，则使用TTBR0，并disable TTBR1. 这是默认情况
  - 如果TTBCR.N > 0， 在VA[31,32-N]都是0时使用TTBR0; 否则使用TTBR1

### arm32-linux的二级页表分为linux版本和硬件版本

单纯硬件角度，ARM32架构 MMU 4KB页面映射是： 

- L1 4096条记录，使用32位虚拟地址的高12位(bit[31:20])作为访问一级页表的索引值，，找到相应的表项，每个表项指向一个二级页表。
- L2 256条记录，使用虚拟地址的次8位(bit[19:12])作为访问二级页表的索引值，得到相应的页表项，从这个页表项中找到20位的物理页面（page）地址。
- 最后将这20位物理页面地址和虚拟地址的低12位拼凑在一起，得到最终的32位物理地址。

因此纯硬件角度所需要的映射表大小应该是：L1 table大小是 4096*sizeof(u32),即16KB。 L2 table大小是256 * sizeof(u32)，即1KB

显然纯硬件角度，下面的结果先记住:

- 从L1视角，它的每条记录看到的内存是1M(2**(32-12))
- 从L2视角，它只有256条记录

由于纯硬件角度L1/L2 table中存放的信息都是地址信息，无法添加额外的控制信息，比如是否dirty，是否accessed。根据“`linux\arch\arm\include\asm\pgtable-2level.h`”中注释说明，linux采用如下方式建立映射表：

- PGD（L1）2048条记录，每条记录是u64. table size是16k
- PMD： 1对1的目录，每个 PMD 只有一个指针. 它不分配任何数据结构，而是存储在PGD
- PTE（L2）512条记录，每条记录是u64。 table size是4k

这导致的结果就是，关系应该是下面的![【软件构建的数据在纯硬件角度的理解】](https://dflund.se/~triad/images/classic-mmu-page-table.jpg)

这并不太好理解,看下面的增加pmd数据的处理应该有助于帮助理解：

- 软件角度往PGD(与PMD合一)可以写2048条记录，每个记录是8字节（pmdp[0]与pmdp[1]），这其实意味者PMD table大小按照4字节来理解就是4096条件记录，与硬件要求完美吻合。

- 

```c++
//linux\arch\arm\include\asm\pgalloc.h
static inline void __pmd_populate(pmd_t *pmdp, phys_addr_t pte,
				  pmdval_t prot)
{
	/**
	 * 以pte 构建对应的上一层的pmd val，这里PTE_HWTABLE_OFF应该没有什么用吧？
	 * 猜测是正上方放metadata，但从代码中没有看到用的地方。[文章](https://people.kernel.org/linusw/arm32-page-tables)提到了meta，但没有进一步详细说
	*/
	pmdval_t pmdval = (pte + PTE_HWTABLE_OFF) | prot;
	pmdp[0] = __pmd(pmdval);
#ifndef CONFIG_ARM_LPAE
	pmdp[1] = __pmd(pmdval + 256 * sizeof(pte_t));
#endif
	flush_pmd_entry(pmdp);
}

//C:\home\rpi-kernels\linux\arch\arm\mm\mmu.c
static pte_t * __init arm_pte_alloc(pmd_t *pmd, unsigned long addr,
				unsigned long prot,
				void *(*alloc)(unsigned long sz))
{
	if (pmd_none(*pmd)) {
		/**
		 * 以pgtable-2level为例，是分配512+512
		*/
		pte_t *pte = alloc(PTE_HWTABLE_OFF + PTE_HWTABLE_SIZE);
		__pmd_populate(pmd, __pa(pte), prot);
	}
	BUG_ON(pmd_bad(*pmd));
	return pte_offset_kernel(pmd, addr);
}


```

从软件角度，分配L1 table大小是2048 * 8 即16KB； 分配L2 table大小是 512 *4，即2KB 

显然硬件角度，软件角度两者建立的表大小都是一样（16KB）。 


## LINUX页面术语

[Chapter 3  Page Table Management](https://www.kernel.org/doc/gorman/html/understand/understand006.html)

- 缩写：
  - PTE:  页表项（page table entry）
  - PGD(Page Global Directory)
  - PUD(Page Upper Directory)
  - PMD(Page Middle Directory)
  - PTE(Page Table)

在linux，每个pte_t指向一个物理页的地址，并且所有的地址都是页（4k）对齐的。因此在32位地址中有PAGE_SHIFT(12)位是空闲的，它可以为PTE的状态位。

- Linux 内核中的通用分页管理机制
	
	```
	PGD
	|---P4D
		|---PUD
			|---PMD
					|---PTE
	```

- ARM32采用下面方式，PTE维护的都是4Kb的page
	
	```
	PGD
	|
	|
	|-------------PMD
					|---PTE
	```

- classic ARM32： 
  	
	每个PGD有2048条记录， 每个PMD有一条记录，每个PTE有512条记录.   即linear address bits是： 11bit(pgd)  + 9bit(pte) + 12bit(page)

	```c++
	//linux\arch\arm\include\asm\pgtable-2level.h
	#define PTRS_PER_PTE		512
	#define PTRS_PER_PMD		1
	#define PTRS_PER_PGD		2048

	#define PTE_HWTABLE_PTRS	(PTRS_PER_PTE)
	#define PTE_HWTABLE_OFF		(PTE_HWTABLE_PTRS * sizeof(pte_t))
	#define PTE_HWTABLE_SIZE	(PTRS_PER_PTE * sizeof(u32))
	```

- LPAE    ARM32:  
  	
	每个PGD有4条记录， 每个PMD有512条记录，每个PTE有512条记录 。LPAE MMU 当然可以将更多的指针放入 PGD，以覆盖高达 1TB 的内存。目前没有 ARM32 架构需要这个，所以我们只是将其降低到内核空间的最大 4GB。4GB 的内核空间内存应该足够每个人使用了。用户空间是另一回事。 即linear address bits是： 2bit(pgd) + 9bit(pmd) + 9bit(pte) + 12bit(page)

	```c++
	//linux\arch\arm\include\asm\pgtable-3level.h
	/*
	* With LPAE, there are 3 levels of page tables. Each level has 512 entries of
	* 8 bytes each, occupying a 4K page. The first level table covers a range of
	* 512GB, each entry representing 1GB. Since we are limited to 4GB input
	* address range, only 4 entries in the PGD are used.
	*
	* There are enough spare bits in a page table entry for the kernel specific
	* state.
	*/
	#define PTRS_PER_PTE		512
	#define PTRS_PER_PMD		512
	#define PTRS_PER_PGD		4

	#define PTE_HWTABLE_PTRS	(0)
	#define PTE_HWTABLE_OFF		(0)
	#define PTE_HWTABLE_SIZE	(PTRS_PER_PTE * sizeof(u64))
	```
因此，当我们说经典的 ARM32 MMU 有 2 级页表时，我们以一种特殊的方式将其呈现给 Linux 为“3 级”，其中中间一个是单个指针，而 LPAE MMU 实际上有 3 级。这确实有些混乱。

- VA(Linear Address) Bit Size Macros概念:
	
	```
			linear Address
	+----------------+-------------+-------------+---------------+
	| PGD            |   PMD       |   PTE       |  OFFSET       |
	+----------------+-------------+-------------+---------------+
	                                             |<--PAGE_SHIFT->|
	                               |<-----PMD_SHIFT------------->|
	                 |<--------------PGDIR_SHIFT---------------->|

	```

	usage sample：
	```c++
	/* to find an entry in a page-table-directory */
    #define pgd_index(addr)		((addr) >> PGDIR_SHIFT)
    #define pgd_offset(mm, addr)	((mm)->pgd + pgd_index(addr))
    ```

- VA(Linear Address) size and mask Macros概念:
	
	```
			linear Address
	+----------------+-------------+-------------+---------------+
	| PGD            |   PMD       |   PTE       |  OFFSET       |
	+----------------+-------------+-------------+---------------+
	|<-------------PAGE_MASK-------------------->|<--PAGE_SIZE-->|
	|<----------PMD_MASK---------->|<------PMD_SIZE------------->|
	|<--PGDIR_MASK-->|<--------------PGDIR_SIZE----------------->|
	```

## linux描述PHY术语

共享存储型多处理机有两种模型

  - UMA: 均匀存储器存取（Uniform-Memory-Access，简称UMA）模型

  - NUMA: 非均匀存储器存取（Nonuniform-Memory-Access，简称NUMA）模型, 所有本地存储器的集合组成了全局地址空间，可被所有的处理机访问。处理机访问本地存储器是比较快的，但访问属于另一台处理机的远程存储器则比较慢，因为通过互连网络会产生附加时延。比较典型的NUMA服务器的例子包括HP的Superdome、SUN15K、IBMp690等。基于NUMA还存在两种改进：COMA（Cache-Only Memory Architecture）和ccNUMA（CacheCoherentNon-UniformMemoryAccess）.

从系统架构来看，目前的商用服务器大体可以分为三类

  - SMP: 对称多处理器结构(SMP：Symmetric Multi-Processor)。各CPU共享相同的物理内存，每个 CPU访问内存中的任何地址所需时间是相同的，因此SMP也被称为一致存储器访问结构(UMA：Uniform Memory Access)。实验证明，SMP服务器CPU利用率最好的情况是2至4个CPU

  - NUMA: 非一致存储访问结构(NUMA：Non-Uniform Memory Access)。

  - MPP : 海量并行处理结构(MPP：Massive Parallel Processing)。基本特征是由多个SMP服务器(每个SMP服务器称节点)通过节点互联网络连接而成，每个节点只访问自己的本地资源(内存、存储等)，是一种完全无共享(Share Nothing)结构.目前业界对节点互联网络暂无标准，如 NCR的Bynet，IBM的SPSwitch.

在liunux， 统一为NUMA。即对UMA系统, 内存就相当于一个只使用一个NUMA节点来管理整个系统的内存. 而内存管理的其他地方则认为他们就是在处理一个(伪)NUMA系统.



Linux把物理内存划分为三个层次（node，zone，page）来管理

  - 存储节点(Node) ：也称为 bank 。CPU被划分为多个节点(node), 内存则被分簇, 每个CPU对应一个本地物理内存, 即一个CPU-node对应一个内存簇bank，即每个内存簇被认为是一个节点
  - 管理区(Zone) ：每个物理内存节点node被划分为多个内存管理区域, 用于表示不同范围的内存, 内核可以使用不同的映射方式映射物理内存
  - 页面(Page) ：	内存被细分为多个页面帧, 页面是最基本的页面分配的单位　

