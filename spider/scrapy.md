  

![scrapyæ¶æ„å›¾](./images/scrapy_architecture_02.png)

![åšå®¢](https://www.cnblogs.com/my8100/tag/scrapy/)

[è‡ªåŠ¨æ‰¹é‡å…³æ³¨å¾®ä¿¡å…¬ä¼—å·ï¼ˆéé€†å‘ï¼‰](https://blog.csdn.net/wnma3mz/article/details/105448808)

[scrapy-selenium](https://github.com/clemfromspace/scrapy-selenium)

https://stackoverflow.com/questions/17975471/selenium-with-scrapy-for-dynamic-page

# ç¯å¢ƒå‡†å¤‡ï¼š

ä»¥ä¸‹éƒ½æ˜¯åŸºäºpython3è¿›è¡Œå¼€å‘ï¼Œ åé¢ä¸å†è¯´æ˜ã€‚ å¯¹anacondaç¯å¢ƒå®‰è£…è¯·è§ â€œpythonç¬”è®°.mdâ€ä¸­windowså®‰è£…éƒ¨åˆ†çš„è¯´æ˜ã€‚

pythonï¼Œipythonä¹Ÿè¯·æŒ‰ç…§è¦æ±‚çš„ç‰ˆæœ¬å·ï¼Œå…¶ä»–ç‰ˆæœ¬ä¼šå‡ºç°åœ¨ipythonä¸­autocompleteæ—¶å‡ºç°è«åçš„debugä¿¡æ¯

```shell
#å®‰è£…python 3.7ï¼Œé«˜ç‰ˆæœ¬å­˜åœ¨time moduleå…¼å®¹é—®é¢˜
conda create -n spider python=3.7
conda activate spider
#å…ˆå®‰è£…ipython
conda install ipython==7.0.1
#å†å®‰è£…scrapy
conda install scrapy=2.1.0

#ç”Ÿæˆé¡¹ç›®
scrapy startproject yourproject_name
scrapy crawl the_name_in_your_class_of_yourcode
```

ä»¥ä¸‹æ˜¯ç¬¬ä¸€ä¸ªæœ€ç®€å•çš„çˆ¬è™«ï¼Œå½“ç„¶å®ƒä»€ä¹ˆä¹Ÿä¸ä¼šåš

```sh
(myenv) c:\tuxueqing\spider>scrapy startproject my
New Scrapy project 'my', using template directory 'C:\prog\Anaconda3\envs\myenv\
lib\site-packages\scrapy\templates\project', created in:
    c:\tuxueqing\spider\my

You can start your first spider with:
    cd my
    scrapy genspider example example.com

(myenv) c:\tuxueqing\spider>
```

[scrapy  å®˜æ–¹æ–‡æ¡£](https://docs.scrapy.org/en/latest)

[scrapy1.5ä¸­æ–‡æ–‡æ¡£](http://www.scrapyd.cn/doc/)

[Scrapy 1.7ä¸­æ–‡æ–‡æ¡£](https://www.osgeo.cn/scrapy/)

https://miyakogi.github.io/pyppeteer/reference.html


# scrapyå‘½ä»¤

[scrapyå‘½ä»¤å®˜æ–¹æ–‡æ¡£](https://docs.scrapy.org/en/latest/topics/commands.html)

scrapyå‘½ä»¤æ”¯æŒå‚æ•°å¦‚ä¸‹æ‰€ç¤ºï¼š

```shell
#åˆ—å‡ºscrapyæ”¯æŒçš„æ‰€æœ‰å‘½ä»¤
(myenv) PS C:\spider> scrapy --help
Scrapy 2.1.0 - no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  bench         Run quick benchmark test
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy
  
  ## ä»¥ä¸‹å‡ ä¸ªå‘½ä»¤ä»…ä»…ä¼šåœ¨ä¸€ä¸ªscrapy projectå†…æ‰ä¼šå‡ºç°
  check         Check spider contracts
  crawl         Run a spider
  edit          Edit spider
  list          List available spiders
  parse         Parse URL (using its spider) and print the results

Use "scrapy <command> -h" to see more info about a command
(myenv) PS C:\tuxueqing\spider>
```

ä¸‹é¢å¯¹æ¯ä¸ªå‘½ä»¤çš„ä½¿ç”¨å°è¯•ä¸‹ï¼ŒåŒæ—¶è®°å½•å…³æ³¨ç‚¹

## scrapy view

`scrapy view [options] <url>`è¿™ä¸ªå‘½ä»¤ä¸»è¦ç›®çš„æ˜¯æŸ¥çœ‹ä¸‹scrapyçœ‹åˆ°çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Œå› ä¸ºæˆ‘ä»¬é€šè¿‡æµè§ˆå™¨è®¿é—®urlä¸scrapyçœ‹åˆ°urlçš„å†…å®¹å¯èƒ½æ˜¯ä¸ä¸€æ ·çš„ã€‚é€šè¿‡å®ƒèƒ½æ›´æ–¹ä¾¿æˆ‘ä»¬ç¼–å†™çˆ¬è™«ã€‚

æ³¨æ„ï¼Œè¿™ä¸ªå‘½ä»¤éœ€è¦åœ¨æœ‰GUIçš„ç³»ç»Ÿä¸­è¿è¡Œï¼Œå› ä¸ºå®ƒéœ€è¦æ‰“å¼€ä¸ªbrowseræ˜¾ç¤ºscrapyè§†è§’çœ‹åˆ°çš„ç½‘é¡µï¼Œå¦‚æœæ˜¯ä¸€ä¸ªæ–‡æœ¬ç»ˆç«¯å°±æ²¡æœ‰æ˜¾ç¤ºäº†ã€

```shell
(myenv) PS C:\tuxueqing\spider\my> scrapy view --help
Usage
=====
  scrapy view [options] <url>

Fetch a URL using the Scrapy downloader and show its contents in a browser

Options
=======
--help, -h              show this help message and exit
--spider=SPIDER         use this spider
--no-redirect           do not handle HTTP 3xx status codes and print response as-is
...
(myenv) PS C:\tuxueqing\spider\my>
```

## scrapy genspider

è¿™ä¸ªå‘½ä»¤æ˜¯ä¸ºæˆ‘ä»¬çš„é¡¹ç›®ç”¨æ¥æ–°å¢åŠ ä¸ªspideræ¨¡æ¿çš„ï¼Œå»ºç«‹è¿™ä¸ªæ¨¡æ¿å¯ä»¥åŸºäºç³»ç»Ÿå†…ç½®çš„æ¨¡æ¿ï¼ˆé€šè¿‡`scrapy genspider --list`å¾—åˆ°ç³»ç»Ÿæ¨¡æ¿åˆ—è¡¨ï¼‰ï¼Œæ¢å¥è¯è¯´å°±æ˜¯ä¸ºæˆ‘ä»¬ç¼–ç å‡å°‘äº›å·¥ä½œé‡çš„ï¼Œæˆ‘ä»¬é“ºä»£ç å¯ä»¥åŸºäºç”Ÿæˆçš„æ¨¡æ¿å¢åŠ æˆ‘ä»¬è‡ªå·±çš„ä»£ç ã€‚

è¯¥å‘½ä»¤æ”¯æŒçš„å‚æ•°å¦‚ä¸‹ï¼š

```shell
(myenv) PS C:\tuxueqing\spider\my> scrapy genspider -h
Usage
=====
  scrapy genspider [options] <name> <domain>

Generate new spider using pre-defined templates

Options
=======
--help, -h              show this help message and exit
--list, -l              List available templates
--edit, -e              Edit spider after creating it
--dump=TEMPLATE, -d TEMPLATE
                        Dump template to standard output
--template=TEMPLATE, -t TEMPLATE
                        Uses a custom template.
--force                 If the spider already exists, overwrite it with the
                        template
...
(myenv) PS C:\tuxueqing\spider\my>
```

### åˆå§‹åŒ–é¡¹ç›®çš„ä¸€ä¸ªçˆ¬è™«

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨çš„å…¸å‹ä¾‹å­ï¼Œåˆ›å»ºçš„æ¨¡æ¿æ–‡ä»¶ä½äºâ€œ`C:\tuxueqing\spider\my\my\spiders\example.py`â€:

```shell
#å…ˆåˆ›å»ºä¸€ä¸ªé¡¹ç›®
(myenv) PS C:\tuxueqing\spider> scrapy startproject my
New Scrapy project 'my', using template directory 'C:\prog\Anaconda3\envs\myenv\
lib\site-packages\scrapy\templates\project', created in:
    C:\tuxueqing\spider\my

You can start your first spider with:
    cd my
    scrapy genspider example example.com
(myenv) PS C:\tuxueqing\spider> cd my
#é‡‡ç”¨ç³»ç»Ÿå†…ç½®crawlæ¨¡æ¿æ–°åˆ›å»ºä¸ªspideræ¨¡æ¿ï¼Œ
(myenv) PS C:\tuxueqing\spider\my> scrapy genspider -t crawl  example example.com
Created spider 'example' using template 'crawl' in module:
  my.spiders.example
(myenv) PS C:\tuxueqing\spider\my>
```

è¯¥"example.py"æ–‡ä»¶çš„å†…å®¹å¦‚ä¸‹ï¼š

```python
# -*- coding: utf-8 -*-
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule

class ExampleSpider(CrawlSpider):
    name = 'example'
    allowed_domains = ['example.com']
    start_urls = ['http://example.com/']

    rules = (
        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        item = {}
        #item['domain_id'] = response.xpath('//input[@id="sid"]/@value').get()
        #item['name'] = response.xpath('//div[@id="name"]').get()
        #item['description'] = response.xpath('//div[@id="description"]').get()
        return item
```
### æŸ¥çœ‹ç³»ç»Ÿæ¨¡æ¿

ä¸‹é¢çš„å‚æ•°æ¼”ç¤ºäº†ï¼š æŸ¥çœ‹ç³»ç»Ÿæœ‰å“ªäº›æ¨¡æ¿ï¼› å°†æ¨¡æ¿å†…å®¹å¯¼å‡ºåˆ°stdoutæŸ¥çœ‹

```shell
# æŸ¥çœ‹æœ‰å“ªäº› ç³»ç»Ÿæ¨¡æ¿
(myenv) PS C:\tuxueqing\spider\my> scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

# æŸ¥çœ‹xmlfeedæ¨¡æ¿çš„å†…å®¹  
(myenv) PS C:\tuxueqing\spider\my> scrapy genspider -d xmlfeed
# -*- coding: utf-8 -*-
from scrapy.spiders import XMLFeedSpider

class $classname(XMLFeedSpider):
    name = '$name'
    allowed_domains = ['$domain']
    start_urls = ['http://$domain/feed.xml']
    iterator = 'iternodes' # you can change this; see the docs
    itertag = 'item' # change it accordingly

    def parse_node(self, response, selector):
        item = {}
        #item['url'] = selector.select('url').get()
        #item['name'] = selector.select('name').get()
        #item['description'] = selector.select('description').get()
        return item

(myenv) PS C:\tuxueqing\spider\my>
```

## scray settings

scrapyé…ç½®å¯ä»¥æ˜¯ç³»ç»Ÿçº§ä¹Ÿå¯ä»¥æ˜¯é¡¹ç›®çº§åˆ«çš„ï¼Œé€šå¸¸éƒ½è®¾ç½®é¡¹ç›®çº§ã€‚é€šè¿‡è¿™ä¸ªå‘½ä»¤å¯ä»¥æŸ¥çœ‹é…ç½®ä¸­é…ç½®é¡¹çš„å€¼ã€‚ åœ¨ä¸‹é¢æ¼”ç¤ºä¸­ï¼Œæ¼”ç¤ºäº†

```shell
(myenv) PS C:\tuxueqing\spider\my> scrapy settings -h
Usage
=====
  scrapy settings [options]

Get settings values

Options
=======
--help, -h              show this help message and exit
--get=SETTING           print raw setting value
--getbool=SETTING       print setting value, interpreted as a boolean
--getint=SETTING        print setting value, interpreted as an integer
--getfloat=SETTING      print setting value, interpreted as a float
--getlist=SETTING       print setting value, interpreted as a list
...
#å°±æ˜¯é¡¹ç›®ä¸­setting.pyé‡Œé¢çš„BOT_NAMEçš„å€¼
(myenv) PS C:\tuxueqing\spider\my> scrapy settings --get=BOT_NAME
my

#å°±æ˜¯é¡¹ç›®ä¸­setting.pyé‡Œé¢çš„ROBOTSTXT_OBEYçš„å€¼
(myenv) PS C:\tuxueqing\spider\my> scrapy settings --get=ROBOTSTXT_OBEY
```

## scrapy runspiderä¸scrapy crawl

`scrapy crawl [options] <spider>`ä¸"`scrapy runspider [options] <spider_file>`"

å®ƒä»¬çš„å·®å¼‚åŒ…æ‹¬ä»¥ä¸‹å‡ ç‚¹ï¼š

- `<spider>` is the project name (åœ¨scrapy projecté¡¹ç›®ç›®å½•ä¸‹é€šè¿‡`scrapy list` å¯ä»¥å¾—åˆ°åˆ—è¡¨).
- `<spider_file>` æ˜¯spiderå…¨è·¯å¾„æ–‡ä»¶åï¼Œis the path to the file that contains the spider.
- `scrapy crawl`æ˜¯ä¾èµ–æœ‰scrapy projectå­˜åœ¨çš„ï¼Œè€Œ`scrapy runspider`ä¸ä¾èµ–

scrapy crawl å‘½ä»¤æ”¯æŒçš„å‚æ•°ï¼š

```shell
(base) PS C:\tuxueqing\spider\my> scrapy crawl -h
Usage
=====
  scrapy crawl [options] <spider>

Run a spider

Options
=======
--help, -h              show this help message and exit
-a NAME=VALUE           set spider argument (may be repeated)
--output=FILE, -o FILE  dump scraped items into FILE (use - for stdout)
--output-format=FORMAT, -t FORMAT
                        format to use for dumping items with -o
...
```

### å¸¸ç”¨arguments

#### -s

`-s CLOSESPIDER_ITEMCOUNT=90`

ç³»ç»Ÿé»˜è®¤é…ç½®é¡¹åˆ—è¡¨ï¼Œå¯ä»¥åœ¨"scrapy\settings\default_settings.py"ä¸­æŸ¥çœ‹åˆ°ï¼Œä¹Ÿå¯ä»¥æŸ¥çœ‹é¡¹ç›®ä¸­çš„"scrapy.cfg"ï¼Œä½†ä¸€èˆ¬è¿™é‡Œé¢æ¯”è¾ƒå°‘ã€‚

#### -o
å¯¹â€œ`-o FILE`â€, ä¼šè‡ªåŠ¨æ ¹æ®shemeä¿å­˜ä¸ºç‰¹å®šæ ¼å¼

ä¾‹å¦‚ï¼š

```shell
#ä¿å­˜ä¸ºjson
$ scrapy crawl basic -o items.json
$ cat items.json
[{"price": ["334.39"], "address": ["Angel, London"], "description":
["website courtâ€¦ offered"], "image_urls": ["../images/i01.jpg"], "title":
["set unique family well"]}]

#ä¿å­˜ä¸ºjsonï¼Œæ•°æ®åœ¨ä¸€è¡Œ
$ scrapy crawl basic -o items.jl
#ä¿å­˜ä¸ºcsv
$ scrapy crawl basic -o items.csv
#ä¿å­˜ä¸ºxml
$ scrapy crawl basic -o items.xml
#é€šè¿‡ftpåè®®
$ scrapy crawl basic -o "ftp://user:pass@ftp.scrapybook.com/items.json "
#é€šè¿‡s3åè®®
$ scrapy crawl basic -o "s3://aws_key:aws_secret@scrapybook/items.json"
```

#### -a

å¯¹`"-a"`å‚æ•°ï¼Œè¿™ä¸ªä¼ é€’çš„æ˜¯spider argumentï¼Œå…¸å‹çš„spider argumenté€šå¸¸å†™åˆ°è¯¥spiderçš„`def start_requests(self, arguments)`  å‚æ•°ä¸­ï¼Œå½“ç„¶å¦‚æœ


### å¸¸è§é—®é¢˜

#### å¯¼å‡ºjsonä¸ºunicodeé—®é¢˜

é€šè¿‡"`-o xxxx.json`"å¯¼å‡ºçˆ¬å–ç»“æœï¼Œå¾€å¾€ä¸­æ–‡çš„å†…å®¹æ˜¾ç¤ºä¸ºasciiåŒ–çš„unicodeï¼Œè¿™çœ‹è¯·æ¥å¾ˆä¸æ–¹ä¾¿ã€‚

è¿™ä¸ªé—®é¢˜çš„åŸå› æ˜¯python jsonåº“çš„`json.dumps`é»˜è®¤å¤„ç†å°±æ˜¯å°†unicodeå®‰è£…asciiåŒ–å¯¼å‡ºã€‚ è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹å¼æ˜¯ï¼š

åœ¨`project settings.py` ä¸­é…ç½® `FEED_EXPORT_ENCODING = 'utf-8'`

## scrapy list

åœ¨scrapy projectä¸­åˆ—å‡ºæœ‰å‡ ä¸ªspiderï¼Œä»¥åŠä»–ä»¬çš„åå­—

ä¾‹å¦‚

```shell
(myenv) PS C:\tuxueqing\spider\my> scrapy list
example
```

## scrapy edit

é€šè¿‡è¯¥å‘½ä»¤æ¥æ‰“å¼€ç¼–è¾‘å™¨å»ç¼–è¾‘spiderï¼Œé€šå¸¸æˆ‘ä»¬ç¼–ç¨‹éƒ½æ˜¯å¸¸å¸¸æ‰“å¼€è¿™ç¼–è¾‘å™¨ï¼Œè¿™ä¸ªæ–¹å¼ä¸å¤ªæœ‰å¿…è¦ï¼Œä½†å¦‚æœéœ€è¦ä¹Ÿå¯ä»¥é€šè¿‡è¿™ä¸ªæ–¹å¼æ¥ä½¿ç”¨ã€‚è¿™ä¸ªå‘½ä»¤ä½¿ç”¨çš„ç¼–è¾‘å™¨æ—¶ç”±ç¯å¢ƒå˜é‡`EDITOR`è®¾ç½®çš„ã€‚

ä½¿ç”¨çœ‹ä¸‹é¢ä¾‹å­

```shell
#æŸ¥çœ‹spider name
(myenv) c:\tuxueqing\spider\my>scrapy list
example

#è®¾ç½®ç¼–è¾‘å™¨ä¸ºcodeï¼Œåœ¨è¿™æ¬¡ç¯å¢ƒä¸­codeæ˜¯vscodeå·²ç»æ”¾å…¥äº†å…¨å±€pathä¸­
(myenv) c:\tuxueqing\spider\my>set EDITOR=code
#ç¼–è¾‘è¯¥spiderï¼Œè¿è¡Œè¯¥å‘½ä»¤åå°†ä¼šçœ‹åˆ°codeè¢«æ‹‰èµ·ï¼Œå¹¶ä¸”æ‰“å¼€äº†example.pyæ–‡ä»¶
(myenv) c:\tuxueqing\spider\my>scrapy edit example
(myenv) c:\tuxueqing\spider\my>
```

## scrapy bench

æµ‹è¯•å½“å‰æœºå™¨çš„æŠ“å–æ€§èƒ½

æ‰§è¡Œè¯¥å‘½ä»¤å¯èƒ½ä¼šå‡ºç°ç±»ä¼¼"`twisted.internet.error.CannotListenError: Couldn't listen on any:8998`"é”™è¯¯ï¼Œå¦‚æœå‡ºç°è¿™ç§é”™è¯¯ï¼Œå»ºè®®é‡‡ç”¨ä¸‹é¢çš„æ–¹å¼æŸ¥å‡ºå ç”¨ç«¯å£çš„è¿›ç¨‹ï¼Œç„¶åæ€æ‰è¿›ç¨‹é‡æ–°å°è¯•

```sh
#æŸ¥æ‰¾å‡ºè¿›ç¨‹id
(myenv) PS C:\tuxueqing\spider> netstat -ano|findstr "8998"
  TCP    0.0.0.0:8998           0.0.0.0:0              LISTENING       3156
#æŸ¥æ‰¾å‡ºè¯¥è¿›ç¨‹idå¯¹åº”æ˜¯å“ªä¸ªè¿›ç¨‹  
(myenv) PS C:\tuxueqing\spider> tasklist | findstr "3156"
SWVisualize.BoostService.     3156 Services                   0     10,608 K
(myenv) PS C:\tuxueqing\spider>
```

ä¸Šé¢æ˜¯windowsä¸‹æŸ¥æ‰¾è¿›ç¨‹çš„æ–¹æ³•ï¼Œç›¸æ¯”ä¸‹liunxæ›´ç®€å•ï¼Œå› ä¸ºâ€œpâ€å‚æ•°ç›´æ¥å°±æ˜¾ç¤ºäº†â€œpidã€nameâ€ä¿¡æ¯ï¼Œé€šå¸¸é‡‡ç”¨`sudo netstat -antup`å‘½ä»¤


```shell
atmel:~$scrapy bench
...
2020-06-01 10:23:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 246351,
 'downloader/request_count': 549,
 'downloader/request_method_count/GET': 549,
 'downloader/response_bytes': 1697683,
 'downloader/response_count': 549,
 'downloader/response_status_count/200': 549,
 'elapsed_time_seconds': 10.565515,
 'finish_reason': 'closespider_timeout',
 'finish_time': datetime.datetime(2020, 6, 1, 2, 23, 38, 792904),
 'log_count/INFO': 20,
 'memusage/max': 55541760,
 'memusage/startup': 55541760,
 'request_depth_max': 21,
 'response_received_count': 549,
 'scheduler/dequeued': 549,
 'scheduler/dequeued/memory': 549,
 'scheduler/enqueued': 10981,
 'scheduler/enqueued/memory': 10981,
 'start_time': datetime.datetime(2020, 6, 1, 2, 23, 28, 227389)}
2020-06-01 10:23:38 [scrapy.core.engine] INFO: Spider closed (closespider_timeout)
atmel:~$
```



































## scrapy fetch

è¿™ä¸ªå‘½ä»¤å¯¹å¯¹åº”urlçš„bodyæˆ–headerä¸‹è½½ä¸‹æ¥è¾“å‡ºåˆ°stdout,æ³¨æ„urlå¿…é¡»å¸¦æœ‰shemeï¼Œå¹¶ä¸”åªæ”¯æŒå•ä¸ªurlï¼Œå¦‚æœè¾“å…¥å¤šä¸ªurlsï¼Œä¸ä¼šæœ‰ç»“æœã€‚

ä½¿ç”¨ä¸¾ä¾‹ï¼š`scrapy fetch --headers --nolog http://www.baidu.com`

```shell
#åˆ—å‡ºfetchæŒ‡ä»¤æ”¯æŒçš„å‚æ•°
(myenv) PS C:\tuxueqing\spider> scrapy fetch -h
Usage
=====
  scrapy fetch [options] <url>
...
Options
=======
--help, -h              show this help message and exit
--spider=SPIDER         use this spider
--headers               print response HTTP headers instead of body
...
```

## scrapy parseTODO

è°ƒè¯•çš„æ—¶å€™ç”¨ï¼Œæ¯”å¦‚æ£€æŸ¥æŸä¸ªç‰¹å®šparseçš„è¾“å‡º

ä¾‹å¦‚

```python
$ scrapy parse --spider=basic http://xxxx

```

## scrapy checkTODO

æ”¯æŒçš„å‚æ•°

```shell

(spider) c:\tuxueqing\spider\my>scrapy check --help
Usage
=====
  scrapy check [options] <spider>

Check spider contracts

Options
=======
--help, -h              show this help message and exit
--list, -l              only list contracts, without checking them
--verbose, -v           print contract tests for all spiders

Global Options
```

`Contracts`åˆçº¦ç±»ä¼¼äºspdierçš„å•å…ƒæµ‹è¯•(unit tests for spiders). èƒ½è®©ä½ å¿«é€Ÿæ£€æŸ¥æ˜¯ä¸æ˜¯æœ‰é—®é¢˜å‘ç”Ÿäº†ï¼Œæ¯”å¦‚ä½ åœ¨å‡ å‘¨å‰å†™äº†ä¸ªçˆ¬è™«ï¼Œä½ å¸Œæœ›å¿«é€Ÿæ£€æŸ¥æ˜¯ä¸æ˜¯çˆ¬è™«é€»è¾‘è¿˜æ˜¯å¯¹çš„ã€‚

`Contracts`åˆçº¦çš„å„ä¸ªåˆçº¦é¡¹æ”¾ç½®åœ¨ç´§éš`docstring`(å‡½æ•°comment)ä¹‹åï¼Œå¹¶ä¸”æ¯è¡Œä»¥â€œ`@`â€å¼€å¤´ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š

```python
def parse(self, response):
    """ This function parses a property page.
    @url http://web:9312/properties/property_000000.html
    @returns items 1
    @scrapes title price description address image_urls
    @scrapes url project spider server date
    """
    ...
```

è¿™ä¸ªä¾‹å­ä¸­çš„`Contracts`åˆçº¦æ˜¯è¡¨è¾¾ï¼š â€œcheck this URL and you should find one item with values on
those fields I enlistâ€. Now if you run scrapy checkï¼ˆ`$scrapy check basic`ï¼‰, it will go and check whether the contracts are satisfied:



## scrapy shellTODO

è¿™ä¸ªåº”è¯¥æ˜¯æœ€å¸¸ç”¨çš„scrapyå‘½ä»¤ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨çš„pythonç»ˆç«¯ç¯å¢ƒï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½•åœ¨æ­£å¸¸python REPLä¸­ä½¿ç”¨çš„ä¸œè¥¿ã€‚å»ºè®®ç¡®ä¿ç³»ç»Ÿå®‰è£…äº†ipythonï¼ˆ`conda install ipython`  æˆ– `pip3 install ipython`ï¼‰ï¼Œå¦‚æœå®‰è£…äº†ï¼Œå®ƒä½¿ç”¨çš„python REPLå°±æ˜¯ipythonï¼Œå¦åˆ™å°±æ˜¯pythoné»˜è®¤ç»ˆç«¯ã€‚

`scrapy shell`æ”¯æŒçš„å‚æ•°å¦‚ä¸‹ï¼š

```shell
(spider) c:\tuxueqing\spider\my>scrapy shell -h
Usage
=====
  scrapy shell [url|file]

Interactive console for scraping the given url or file. Use ./file.html syntax
or full path for local file.

Options
=======
--help, -h              show this help message and exit
-c CODE                 evaluate the code in the shell, print the result and
                        exit
--spider=SPIDER         use this spider
--no-redirect           do not handle HTTP 3xx status codes and print response
                        as-is

Global Options
--------------
--logfile=FILE          log file. if omitted stderr will be used
--loglevel=LEVEL, -L LEVEL
                        log level (default: DEBUG)
--nolog                 disable logging completely
--profile=FILE          write python cProfile stats to FILE
--pidfile=FILE          write process ID to FILE
--set=NAME=VALUE, -s NAME=VALUE
                        set/override setting (may be repeated)
--pdb                   enable pdb on failure
```

### å¸¸ç”¨arguments

å…¶ä¸­æœ€é‡è¦çš„ä¸¤ä¸ªargumentæ˜¯:

- -s USER_AGENT="Mozilla/5.0" : è®¾ç½®USER_AGENT
- -pdb    ï¼šenable interactive debugging in case of exceptions.

### å¸¸è§é—®é¢˜

#### scrapyshellè«ådebugä¿¡æ¯
```sh
(myenv) C:\Users\atmel>scrapy shell
2020-05-25 18:59:21 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)
...
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x0000000004C31940>
[s]   item       {}
[s]   settings   <scrapy.settings.Settings object at 0x0000000004C31850>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
>>>
>>> fetch("http://www.baidu.com")
2020-05-25 19:05:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.baidu.com> (referer: None)
>>> response.css("title")
[<Selector xpath='descendant-or-self::title' data='<title>ç™¾åº¦ä¸€ä¸‹ï¼Œä½ å°±çŸ¥é“</title>'>]
>>>
```

è¿™é‡Œæœ‰ä¸ªå¾ˆå¥‡æ€ªçš„é—®é¢˜ï¼Œåœ¨`scrapy shell`ä¸­ï¼Œé€šè¿‡TABæ¥è‡ªåŠ¨å®Œæˆæ˜¯ï¼Œå‡ºç°è«åçš„debugä¿¡æ¯ã€‚ä»ç½‘ä¸Šä¿¡æ¯çœ‹è¿™ä¸ªé—®é¢˜æ²¡æœ‰è§£å†³ï¼Œå¯ä»¥å…ˆé€šè¿‡ä¸‹é¢çš„æ–¹å¼è§„é¿æ‰:

æ–¹å¼1ï¼š ä¸é™åˆ¶å®‰è£…pythonã€ipythonç‰ˆæœ¬

```python
In [3]: import logging
In [4]: logging.getLogger('parso').setLevel(logging.WARNING)
```

æ–¹å¼2ï¼š å®‰è£…`pip install ipython==7.0.1`ï¼Œè¿™ä¸ªç‰ˆæœ¬çš„ipythonæ²¡æœ‰è¿™ä¸ªé—®é¢˜ï¼Œä½†å¯¹åº”çš„pythonè¦å›é€€åˆ°3.7

å¦‚æœé‡‡ç”¨äº†å‰é¢ç¯å¢ƒå‡†å¤‡ä¸­çš„ç‰ˆæœ¬ï¼Œé‚£è¿™ä¸ªé—®é¢˜ä¸ä¼šå‡ºç°ã€‚å¦‚æœä¸æ˜¯ï¼Œå»ºè®®é‡‡ç”¨æ–¹å¼1ï¼Œè™½ç„¶æœ‰äº›éº»çƒ¦ï¼Œä½†è‡³å°‘ä¸ä¼šå½±å“env


### é…ç½®é€‰æ‹©pythonç»ˆç«¯

åœ¨`scrapy shell`ä¸­ä½¿ç”¨ä»€ä¹ˆpythonç»ˆç«¯ï¼Œç³»ç»Ÿä¹Ÿæä¾›äº†è‡ªå·±é…ç½®çš„èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨ä½ çš„çˆ¬è™«é¡¹ç›®çš„`scrapy.cfg`ä¸­å¢åŠ ä¸‹é¢çš„è¯­å¥ï¼Œå°†å¯¼è‡´åœ¨è¯¥çˆ¬è™«é¡¹ç›®æ‰§è¡Œ`scrapy shell`ä½¿ç”¨çš„æ˜¯bpythonç»ˆç«¯:

```conf
[settings]
shell = bpython
```

å¦å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡SCRAPY_PYTHON_SHELLç¯å¢ƒæ¥è®¾ç½®ã€‚

### spiderè°ƒç”¨scrapy shell

æœ‰æ—¶ï¼Œå¸Œæœ›åœ¨spiderä¸­æ£€æŸ¥åˆ°è¾¾çš„responseï¼Œè¿™å¯ä»¥é€šè¿‡`scrapy.shell.inspect_response`åŠŸèƒ½æ¥å®ç°

```python
class MySpider(scrapy.Spider):
...
    def parse(self, response):
        # We want to inspect one specific response.
        if ".org" in response.url:
            from scrapy.shell import inspect_response
            inspect_response(response, self)
```
ä¸Šé¢çš„ä¾‹å­å°†å¯¼è‡´æ§åˆ¶è½¬ç§»åˆ°`scrapy shell`,è¿™æ—¶å°±å¯ä»¥å’Œé€šå¸¸ä¸€æ ·åœ¨`scrapy shell`ä¸­æ£€æŸ¥response.æœ€åï¼Œæ‚¨æŒ‰Ctrl-Dï¼ˆæˆ–Windowsä¸­çš„Ctrl-Zï¼‰é€€å‡ºå¤–å£³ç¨‹åºå¹¶ç»§ç»­çˆ¬ç½‘.


# scrapyé€‰æ‹©å™¨

ä»ç½‘é¡µä¸­æå–æ•°æ®å¸¸ç”¨çš„åº“åŒ…æ‹¬ï¼š BeautifulSoupï¼Œlxmlã€‚ scrapyåˆ›å»ºäº†è‡ªå·±çš„æå–å™¨Scrapy Selectorsï¼ŒScrapy Selectorså®ƒåŸºäº[Parsel](https://parsel.readthedocs.io/en/latest/)ã€‚ä¸‹é¢å¯¹scrapyæå–å™¨è¿›è¡Œä»‹ç»ã€‚

[scrapyé€‰æ‹©å™¨å®˜ç½‘](https://docs.scrapy.org/en/latest/topics/selectors.html#topics-selectors-ref)

scrapyé€‰æ‹©å™¨æ”¯æŒ: cssé€‰æ‹©å™¨ï¼ˆSelector.cssï¼‰ã€xpathé€‰æ‹©å™¨ï¼ˆSelector.xpathï¼‰ã€æ­£åˆ™ï¼ˆSelector.reï¼‰


ä¸‹é¢çš„ä¾‹å­æ˜¯ç›´æ¥ä½¿ç”¨Scrapy Selectorsçš„ä¾‹å­ï¼š

```shell
In [11]: from scrapy.selector import Selector
In [12]: body = '<html><body><span>good</span></body></html>'
In [13]: Selector(text=body).xpath('//span/text()').get()
Out[13]: 'good'
```

### cssé€‰æ‹©å™¨

[scrapy selectorå®˜ç½‘](https://docs.scrapy.org/en/latest/topics/selectors.html)
[scrapyæå–æ•°æ®ä¹‹ï¼šCSSé€‰æ‹©å™¨ é«˜çº§ç”¨æ³•](https://blog.csdn.net/weixin_42547344/article/details/89306084)

åœ¨chrome devtoolsçš„consloeä¸­å¯ä»¥ç›´æ¥æ‰§è¡ŒcssæŸ¥è¯¢ï¼Œä¾‹å¦‚â€œ`$(".wrap .leftDiv > .demo")`â€

`response.css(query)` æ˜¯`response.selector.css(query)`çš„ç®€å†™å½¢å¼ã€‚å…¶ä¸­å‚æ•°`query` æ˜¯åŒ…å«è¦åº”ç”¨çš„CSSé€‰æ‹©å™¨çš„å­—ç¬¦ä¸²


#### æå–å±æ€§

æ ‡å‡†CSSé€‰æ‹©å™¨ä¸æ”¯æŒé€‰æ‹©æ–‡æœ¬èŠ‚ç‚¹æˆ–å±æ€§å€¼ã€‚ä½†Scrapyï¼ˆparselï¼‰å®ç°äº†ä¸¤ä¸ªéæ ‡å‡†çš„ä¼ªå…ƒç´ ï¼š

- æå–å±æ€§ï¼šâ€œ`é€‰æ‹©å™¨::attr(å±æ€§å)`â€ï¼Œ
- æå–æ ‡ç­¾å†…å®¹ï¼šâ€œ`é€‰æ‹©å™¨::text`â€

#### æ’é™¤

æœ‰æ—¶æå–çš„é€»è¾‘æ˜¯éœ€è¦åœ¨ä¸€æ®µå†…å®¹ä¸­å»æ‰ç‰¹å®šçš„å†…å®¹ï¼Œcssé€‰æ‹©å™¨æœ¬èº«æ²¡æœ‰è¿™æ–¹é¢çš„è¯­æ³•æ”¯æ’‘ï¼Œè¿™æ—¶å€™éœ€è¦å˜é€šåšä¸‹ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­

  ```python
  # è·å–æ­£æ–‡,
  content = s.css("article .sPACont").get()
  rubbish = s.css(
      "article .sPACont   .sHFixedA, .sPMc, .sForPER , a[onclick]").getall()
  # æ’é™¤
  for rb in rubbish:
      content = content.replace(rb, "")
  ```

#### è½¬æ¢åœ°å€

åœ¨çˆ¬å–æ•°æ®åç»å¸¸æœ‰éœ€è¦å°†ç›¸å¯¹åœ°å€è½¬æ¢ä¸ºç»å¯¹åœ°å€ç±»ä¼¼çš„æ›¿æ¢è¦æ±‚ï¼Œä¸‹é¢æ˜¯ä¸€ç§æ–‡æœ¬æ›¿æ¢å®ç°æ–¹å¼ï¼š

  ```python
  # å°†æ­£æ–‡ä¸­çš„å›¾ç‰‡é“¾æ¥è½¬æˆç»å¯¹åœ°å€
  images = Selector(text=content).css(" .sPACont img").getall()
  # å°†æ­£æ–‡ä¸­çš„å¼•ç”¨é“¾æ¥è½¬æˆç»å¯¹åœ°å€
  anchors = Selector(text=content).css(" .sPACont a").getall()
  conv = []
  # æ„é€ æ›¿æ¢æ˜ å°„-img
  for i in images:
      t = Selector(text=i)
      old = t.css("img::attr(src)").get()
      new = urljoin(response.url, old)
      i_new = i.replace(old, new)
      conv.append((i, i_new))
  # æ„é€ æ›¿æ¢æ˜ å°„-anchor
  for i in anchors:
      t = Selector(text=i)
      old = t.css("a::attr(href)").get()
      new = urljoin(response.url, old)
      i_new = i.replace(old, new)
      conv.append((i, i_new))
  # æ›¿æ¢
  for o, n in dict(conv).items():
      content = content.replace(o, n)
  ```

### æ­£åˆ™é€‰æ‹©å™¨

`response.reï¼ˆregexï¼Œreplace_entities = Trueï¼‰`æ˜¯`response.selector.reï¼ˆregexï¼Œreplace_entities = Trueï¼‰`çš„ç®€å†™å½¢å¼ã€‚å…¶ä¸­`regex`å¯ä»¥æ˜¯å·²ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œä¹Ÿå¯ä»¥æ˜¯å°†ä½¿ç”¨ç¼–è¯‘ä¸ºæ­£åˆ™è¡¨è¾¾å¼çš„å­—ç¬¦ä¸²`re.compile(regex)`ã€‚

`re_firstï¼ˆregexï¼Œé»˜è®¤= Noneï¼Œreplace_entities = Trueï¼‰`.åº”ç”¨ç»™å®šçš„æ­£åˆ™è¡¨è¾¾å¼å¹¶è¿”å›åŒ¹é…çš„ç¬¬ä¸€ä¸ªunicodeå­—ç¬¦ä¸²ã€‚å¦‚æœä¸åŒ¹é…ï¼Œåˆ™è¿”å›é»˜è®¤å€¼ï¼ˆ`None`å¦‚æœæœªæä¾›å‚æ•°ï¼‰ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œå­—ç¬¦å®ä½“å¼•ç”¨ç”±å…¶å¯¹åº”çš„å­—ç¬¦æ›¿æ¢ï¼ˆ`&amp;`å’Œé™¤å¤–`&lt;`ï¼‰ã€‚é€šè¿‡`replace_entities`ä½œä¸ºFalseå…³é—­è¿™äº›æ›¿ä»£å¼€å…³ã€‚

### xpathé€‰æ‹©å™¨

åœ¨chromeçš„devtools consoleä¸­ç›´æ¥å¯ä»¥æ‰§è¡ŒxpathæŸ¥è¯¢ï¼Œä¾‹å¦‚â€œ`$x('//h1')`â€

XPath ä½¿ç”¨è·¯å¾„è¡¨è¾¾å¼åœ¨ XML æ–‡æ¡£ä¸­è¿›è¡Œå¯¼èˆª


# spiderç±»ä¸å­ç±»

[scrapy spiderså®˜ç½‘](https://docs.scrapy.org/en/latest/topics/spiders.html)

`scrapy.spiders.Spider`æ˜¯æœ€åŸºç¡€çš„çˆ¬è™«ã€‚ æ˜¯æ‰€æœ‰å…¶ä»–çˆ¬è™«çš„çˆ¶ç±»

## Spider

æ˜¯`scrapy.spiders.Spider`ï¼Œé€šè¿‡`scrapy genspider -t basic`åˆ›å»ºçš„çˆ¬è™«å°±æ˜¯å®ƒã€‚

## Generic Spiders

ç»§æ‰¿`Spider`è€Œæ¥çš„é€šç”¨çˆ¬è™«ç±»ï¼Œå®ƒå…¸å‹çš„çˆ¬å–æ˜¯ï¼š

```python
def parse(self, response):
    # Get the next index URLs and yield Requests
    next_selector = response.xpath('//*[contains(@class,"next")]//@href')
    for url in next_selector.extract():
      yield Request(urlparse.urljoin(response.url, url))
```

### CrawlSpider

`scrapy.spiders.CrawlSpider`ç»§æ‰¿è‡ª`scrapy.spiders.Spider`ï¼Œ å®ƒå®ç°çš„`parse() method`ä½¿ç”¨`rules variable`å¯ä»¥æ–¹ä¾¿çš„è¿›è¡Œlinkè·Ÿè¸ªã€‚ ä½¿ç”¨`Rule`ï¼Œscapyä¼šè‡ªåŠ¨å‘å‡º`Request`ï¼Œä¸éœ€è¦æˆ‘ä»¬æ‰‹åŠ¨å‘ã€‚

ä¾‹å¦‚ï¼Œå‡è®¾åœ¨starturlé¡µé¢ä¸­å°±å¯ä»¥æ‰¾åˆ°ç¿»é¡µé“¾æ¥:

```python
class Example2Spider(CrawlSpider):
    ...
    #å®šä¹‰â€æå–åŠ¨ä½œâ€œ
    rules = (
      #è·å–ç¿»é¡µé“¾æ¥  callbackä¸ºNone,å› æ­¤followä¸ºTrue
      Rule(LinkExtractor(restrict_xpaths='...')),
      #æå–å“åº”é¡µé¢
      Rule(LinkExtractor(restrict_xpaths='...'),callback='parse_item')
    )
    def parse_item(self, response):
        pass 
```

é™¤éè®¾ç½®äº†callbackï¼Œå¦åˆ™è§„åˆ™å°†è·Ÿéšæå–çš„urlï¼Œè¿™æ„å‘³ç€å®ƒå°†æ‰«æç›®æ ‡é¡µé¢ä»¥æŸ¥æ‰¾é¢å¤–çš„é“¾æ¥å¹¶è·Ÿéšå®ƒä»¬ã€‚å¦‚æœè®¾ç½®äº†å›è°ƒï¼Œåˆ™è§„åˆ™ä¸ä¼šè·Ÿéšç›®æ ‡é¡µä¸­çš„é“¾æ¥ã€‚å¦‚æœæ‚¨å¸Œæœ›å®ƒè·Ÿéšé“¾æ¥ï¼Œæ‚¨åº”è¯¥ä»å›è°ƒæ–¹æ³•è¿”å›/æ”¾å¼ƒ`Request(follow-link)`ï¼Œæˆ–è€…å°†Ruleï¼ˆï¼‰çš„followå‚æ•°è®¾ç½®ä¸ºtrueã€‚
unless callback is set, a Rule will follow the extracted URLs, which means that it will scan target pages for extra links and follow them. If a callback is set, the Rule wonâ€™t follow the links from target pages. If you would like it to follow links, you should either return/yield them from your callback method, or set the follow argument of Rule() to true.

å¦‚æœåœ¨starturlä¸­æ²¡æœ‰ç¿»é¡µé“¾æ¥çš„ï¼Œé‚£åº”è¯¥åœ¨`pasrse_item`ä¸­ï¼Œå¢åŠ ç±»ä¼¼ä¸‹é¢çš„ä»£ç 

```python
def parse_item(self,response):
    ...
    yield item
    #ç¿»é¡µä»£ç 
    next_page_url=...
    yield scrapy.Request(next_page_url,callback=self.parse_item)
```



#### Ruleè§„åˆ™è§£æå™¨

æ ¹æ®é“¾æ¥æå–å™¨ä¸­æå–åˆ°çš„é“¾æ¥ï¼Œæ ¹æ®æŒ‡å®šè§„åˆ™æå–è§£æå™¨é“¾æ¥ç½‘é¡µä¸­çš„å†…å®¹ã€‚

```python
Rule(LinkExtractor(...),  #æŒ‡å®šé“¾æ¥æå–å™¨
     callback='parse_item', #æŒ‡å®šè§„åˆ™è§£æå™¨è§£ææ•°æ®çš„è§„åˆ™ï¼ˆå›è°ƒå‡½æ•°ï¼‰
     follow=True, #æ˜¯å¦å°†é“¾æ¥æå–å™¨ç»§ç»­ä½œç”¨åˆ°é“¾æ¥æå–å™¨æå–å‡ºçš„é“¾æ¥ç½‘é¡µä¸­ã€‚
                  #å½“callbackä¸ºNone,followçš„é»˜è®¤å€¼ä¸ºtrue
                  #å¦‚æœæ˜¯Falseåªè·å–èµ·å§‹urlé¡µé¢ä¸­è¿æ¥
     cb_kwargs= ,
     process_link= ,
     process_request = ,
     errback = ...)             
```


#### LinkExtractorsé“¾æ¥æå–å™¨

```python
LinkExtractor(
ã€€ã€€allow=r'Items/'ï¼Œ# æ»¡è¶³æ‹¬å·ä¸­â€œæ­£åˆ™è¡¨è¾¾å¼â€çš„å€¼ä¼šè¢«æå–ï¼Œå¦‚æœä¸ºç©ºï¼Œåˆ™å…¨éƒ¨åŒ¹é…ã€‚
ã€€ã€€deny=xxx,  # æ»¡è¶³æ­£åˆ™è¡¨è¾¾å¼çš„åˆ™ä¸ä¼šè¢«æå–ã€‚
ã€€ã€€restrict_xpaths=xxx, # æ»¡è¶³xpathè¡¨è¾¾å¼çš„å€¼ä¼šè¢«æå–
ã€€ã€€restrict_css=xxx, # æ»¡è¶³cssè¡¨è¾¾å¼çš„å€¼ä¼šè¢«æå–
ã€€ã€€deny_domains=xxx, # ä¸ä¼šè¢«æå–çš„é“¾æ¥çš„domainsã€‚ã€€
)
```




### XMLFeedSpider

`scrapy.spiders.XMLFeedSpider`,

### CSVFeedSpider

`scrapy.spiders.CSVFeedSpider`, is very similar to the XMLFeedSpider, except that it iterates over rows, instead of nodes. The method that gets called in each iteration is parse_row().

#### ä½¿ç”¨basicè§£æcsvè¾“å…¥

```python
class FromcsvSpider(scrapy.Spider):
  name = "fromcsv"
  def start_requests(self):
    with open("todo.csv", "rU") as f:
      reader = csv.DictReader(f)
      for line in reader:
        request = Request(line.pop('url'))
        request.meta['fields'] = line
        yield request

  def parse(self, response):
    item = Item()
    l = ItemLoader(item=item, response=response)
    for name, xpath in response.meta['fields'].iteritems():
        if xpath:
          item.fields[name] = Field()
          l.add_xpath(name, xpath)
    return l.load_item()
```

#### ä½¿ç”¨CSVFeedSpider

ç›´æ¥ä½¿ç”¨æ¨¡æ¿"`scrapy genspider -t csvfeed`â€œç”Ÿæˆçš„spider,æ³¨æ„ä¸‹é¢ä¾‹å­çš„ä¸åŒï¼Œç¬¬äºŒä¸ªæ˜¯ä»¥æœ¬åœ°æ–‡ä»¶ä½œä¸ºè¾“å…¥

```python
# start_urls = ['http://com.example/feed.csv']
# start_urls = ['file:///home/user/feed.csv']
```

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨æœ¬åœ°æ–‡ä»¶çš„ä¾‹å­ï¼Œ[how-to-use-scrapy-csvfeedspider-to-crawl-a-feed-that-has-commas-in-its-values](https://stackoverflow.com/questions/23522095/how-to-use-scrapy-csvfeedspider-to-crawl-a-feed-that-has-commas-in-its-values)
```python
# -*- coding: utf-8 -*-
from scrapy.spider import Spider
from scrapy.selector import Selector
from stackoverflow23429315.items import DemoItem
from scrapy.contrib.spiders import CSVFeedSpider
from scrapy import log

# Testing csv file:
# 1,"John, Doe","1234 Main Street, APT A","2nd Floor",John.Doe@test.com
# 2,"John2, Doe","1234 Main Street, APT A","2nd Floor",John.Doe@test.com
# 3,'John3, Doe','1234 Main Street, APT A','2nd Floor',John.Doe@test.com
# 4,'John4, Doe','1234 Main Street, APT A','2nd Floor',John.Doe@test.com

class DmozSpider(CSVFeedSpider):
    name = 'csvFeedTest'        
    start_urls = ['file:////home/vagrant/labs/stackoverflow23429315/test.csv']
    delimiter = ','
    headers = ['id', 'name', 'address1', 'address2', 'email']

    def parse_row(self, response, row):
        log.msg('Hi, this is a row!: %r' % row)

        item = DemoItem()
        item['id'] = row['id']
        item['name'] = row['name']
        item['address1'] = row['address1']
        item['address2'] = row['address2']
        item['email'] = row['email']
        return item

from scrapy.item import Item, Field

class DemoItem(Item):
    id = Field()
    name = Field()
    address1 = Field()
    address2 = Field()
    email = Field()
```



### SitemapSpider

`scrapy.spiders.SitemapSpider`,  allows you to crawl a site by discovering the URLs using Sitemaps

## åœ¨responseé—´ä¼ é€’æ•°æ®

é€šè¿‡`meta` æ–¹å¼,ä¸‹é¢æ¼”ç¤ºäº†åœ¨è¯·æ±‚ä¸­æºå¸¦ï¼Œä»¥åŠåœ¨ç›®çš„åœ°æå–å‡ºæ¥

```python
yield Request(url, meta={"title": title},callback=self.parse_item)
```

```python
def parse_item(self, response):
   l.add_value('title', response.meta['title'],
         MapCompose(unicode.strip, unicode.title))
```

## å¯åŠ¨çˆ¬è™«

### å‘½ä»¤è¡Œä¼ é€’starturl
```python
class MySpider(BaseSpider):
    name = 'my_spider'    
    def __init__(self, *args, **kwargs):
        """
        æ”¯æŒå‘½ä»¤è¡Œä¼ é€’starturl
        e.gï¼š scrapy crawl my_spider -a start_urls="http://example1.com,http://example2.com"
        """
        super(MySpider,self).__init__(*args, **kwargs)
        self.start_urls = kwargs.get('start_urls').split(',')

#And start it like: scrapy crawl my_spider -a start_urls="http://some_url"
```

### crawlæˆ–runspider

```python
# é€šè¿‡scrapy projectï¼Œå¯åŠ¨çˆ¬è™«ï¼Œ<spider>å¯é€šè¿‡scrapy listæŸ¥æ‰¾å¯ç”¨çš„åå­—
~$scrapy crawl [options] <spider>

# é€šè¿‡spideræ–‡ä»¶å¯åŠ¨çˆ¬è™«ï¼Œä¸ä¾èµ–scrapy project
$scrapy runspider [options] <spider_file>
```

### å•ç‹¬å¯åŠ¨æ–‡ä»¶

æˆ–å•ç‹¬å†™ä¸ªçˆ¬è™«å¯åŠ¨pythonæ–‡ä»¶

```python
# å•ç‹¬çš„å¯åŠ¨æ–‡ä»¶ï¼Œé€šè¿‡scrapyå‘½ä»¤è¡Œå¯åŠ¨çˆ¬è™«
scrapy.cmdline.execute("scrapy crawl myspider".split())
```

# items

[scrapy items å®˜ç½‘](https://docs.scrapy.org/en/latest/topics/items.html)

### ç®€å•ä½¿ç”¨

Scrapyæä¾›äº†Itemç±»ã€‚Itemå¯¹è±¡æ˜¯ç§ç®€å•çš„å®¹å™¨ï¼Œä¿å­˜äº†çˆ¬å–å¾—åˆ°çš„æ•°æ®ã€‚æ¯”å¦‚ä¸‹é¢å°±æ˜¯ä¸€ä¸ªæœ€ç®€å•çš„ä¾‹å­ï¼ŒSpideråˆ†æçˆ¬å–åˆ°æ•°æ®ï¼Œç»„è£…ç»“æ„åŒ–æ•°æ®`MyItem`

```python
class MySpider(scrapy.Spider):
...
    def parse(self, response):
        for h3 in response.xpath('//h3').getall():
            yield MyItem(title=h3)
```

æ¯”å¦‚ï¼Œä¸‹é¢å®šä¹‰PropertiesItemçš„æ–¹å¼ï¼Œæ˜¯ä¸€ä¸ªè¾ƒä¸ºæ ‡å‡†çš„Itemå½¢å¼

```python
from scrapy.item import Item, Field
class PropertiesItem(Item):
    # Primary fields
    title = Field()
    price = Field()
    description = Field()
    address = Field()
    image_urls = Field()
    # Calculated fields
    images = Field()
    location = Field()
    # Housekeeping fields
    url = Field()
    project = Field()
    spider = Field()
    server = Field()
    date = Field()
```

### æ„é€ å¤æ‚ç»“æ„

é¦–å…ˆè¦è®°ä½Item å¯¹è±¡æ˜¯ç§ç®€å•çš„å®¹å™¨ã€‚å…¶æä¾›äº† ç±»ä¼¼äºè¯å…¸(dictionary-like) çš„APIã€‚è¿™æ„å‘³ç€ä½ ä»¬å­˜æ”¾ä»€ä¹ˆå®ƒæ˜¯ä¸å…³å¿ƒçš„ã€‚æ¯”å¦‚å¯¹`url = Field()`å­—æ®µï¼Œé‡Œé¢æ˜¯å­˜æ”¾`list`ï¼Œè¿˜æ˜¯`dict`ï¼Œè¿˜æ˜¯`str`,å®Œå…¨æœ‰ä¸šåŠ¡è‡ªå·±å†³å®šã€‚

ä¸¾ä¾‹æ¥è¯´ï¼Œæˆ‘ä»¬å®šä¹‰äº†å¦‚ä¸‹çš„Itemï¼š

```python
class CcherePageItem(scrapy.Item):
    #
    url = scrapy.Field()
    articles = scrapy.Field()


class CchereArticleItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    url = scrapy.Field()
    # æ˜¯å›å¤å“ªç¯‡æ–‡ç« 
    anstourl = scrapy.Field()
    title = scrapy.Field()
    # æ–‡ç« æ­£æ–‡
    c = scrapy.Field()
    # åœ¨cchereå…¨çœ‹æ–¹å¼ä¸‹"/thread/xx"å…·å¤‡çš„å±‚çº§ä¿¡æ¯
    lvl = scrapy.Field()
    auth = scrapy.Field()
    date = scrapy.Field()
```

åœ¨ä½¿ç”¨çš„æ—¶å€™æ˜¯ä¸‹é¢æ–¹å¼ï¼š

```python
output_p = CcherePageItem()
output_p['url'] = response.url

# å­˜æ”¾å¯¼å‡ºçš„æ–‡ç« 
output_articles = []
for target_list in expression_list:
    article = CchereArticleItem(...)
    output_articles.append(article)

output_p['articles'] = output_articles
yield output_p
```

è¿™æœ€åçˆ¬å–å‡ºæ¥çš„æ•°æ®å°±ç±»ä¼¼ä¸‹é¢ï¼š

```json
[
    {
        "url": "https://www.talkcc.net/thread/4519373/7",
        "articles": [
            {
                "lvl": "6",
                "title": "å¾ˆå¥‡æ€ªçš„æ˜¯å•†ä»£",
                ...
            },
            {
                "lvl": "8",
                "title": "ä¹Ÿè®¸å¤ä»£é‚£æ˜¯ä¸˜é™µï¼Œç°åœ¨è¢«é»„æ²³éƒ½æ·¤å¹³äº†ï¼Ÿ",
                ...
            },
            ...
        ]
    },
    ...

```


### Item Loaders

[item loadså®˜ç½‘](http://doc.scrapy.org/en/latest/topics/loaders.html)

åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­`MyItem(title=h3)`ï¼Œæˆ‘ä»¬æ˜¯ç›´æ¥é‡‡ç”¨`Itemsç±»`çš„`dict-like api` æ¥è¿›è¡Œitemå¡«å……çš„. ä½†scrapyåŒæ—¶æä¾›äº†`Item Loader`æ–¹å¼å¯¹itemè¿›è¡Œå¡«å……ã€‚


ä¾‹å­ï¼š

```python
    def parse(self, response):
    l = ItemLoader(item=PropertiesItem(), response=response)
    l.add_xpath('title', '//*[@itemprop="name"][1]/text()')
    ...
    return l.load_item()
```


# item pipelines

æ¯ä¸ª`item pipeline`ç»„ä»¶ï¼ˆæœ‰æ—¶ä¹Ÿç§°ä¸ºâ€œé¡¹ç›®ç®¡é“â€ï¼‰éƒ½æ˜¯ä¸€ä¸ªå®ç°ç®€å•æ–¹æ³•çš„Pythonç±»ã€‚ä»–ä»¬æ¥æ”¶åˆ°ä¸€ä¸ª`item`å¹¶å¯¹å…¶æ‰§è¡Œæ“ä½œ,æ¯”å¦‚åå¤„ç†ï¼Œæ¸…æ´—ï¼Œå­˜å‚¨...ã€‚è¿˜å†³å®šè¯¥`item`æ˜¯å¦åº”ç»§ç»­é€šè¿‡ç®¡é“æˆ–è¢«åˆ é™¤å¹¶ä¸å†å¤„ç†ã€‚

æ¯ä¸ª`item pipeline`ç»„ä»¶éƒ½å¿…é¡»å®ç°`def process_item(self, item, spider):`ã€‚

é€šå¸¸åœ¨pipelineå®ç°å¯¹itemè¿›è¡Œæ¸…ç†ï¼Œå°†æ•°æ®æœ€ç»ˆå½’æ¡£ï¼ˆå‘é€ç»™ç¬¬ä¸‰æ–¹ï¼Œå†™æ–‡ä»¶ï¼Œå†™æ•°æ®åº“...ï¼‰

ä¸‹é¢è¿™ä¸ªä¾‹å­æ˜¯ä¸€ä¸ªè¾ƒä¸ºå¤æ‚çš„ä¾‹å­ï¼Œå°†itemå†™å…¥MongoDB
```python
import pymongo

class MongoPipeline:

    collection_name = 'scrapy_items'

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    #è¿”å›pipeline instance
    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )
    #open spideråå°†è°ƒç”¨è¯¥æ–¹æ³•
    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]
    #close spideråå°†è°ƒç”¨è¯¥æ–¹æ³•
    def close_spider(self, spider):
        self.client.close()
    #å¿…é¡»æ‰§è¡Œä»¥ä¸‹æ“ä½œä¹‹ä¸€ï¼šè¿”å›å¸¦æœ‰æ•°æ®çš„å­—å…¸ï¼Œè¿”å›ä¸€ä¸ªItem ï¼ˆæˆ–ä»»ä½•åä»£ç±»ï¼‰å¯¹è±¡ï¼Œ
    #è¿”å› Deferredæˆ–å¼•å‘ DropItemå¼‚å¸¸ã€‚åˆ é™¤çš„é¡¹ç›®ä¸å†ç”±å…¶ä»–ç®¡é“ç»„ä»¶å¤„ç†ã€‚
    
    def process_item(self, item, spider):
        self.db[self.collection_name].insert_one(dict(item))
        return item
        #raise DropItem("Missing price in %s" % item)
```        

# Feed exports


# Spiders Contracts

åœ¨å®˜ç½‘çš„æœ¯è¯­ä¸­æœ‰contractä¸contractsï¼Œå¯¹å®ƒä»¬çš„ç¿»è¯‘ï¼Œå…ˆå®šä¸ºï¼š contractåˆçº¦é¡¹ï¼Œ contractsåˆçº¦ã€‚

å…¶ä¸­contractsåˆçº¦æ˜¯æŒ‡å…³è”äº† contractåˆçº¦é¡¹çš„callbackå‡½æ•°ã€‚ ä¾‹å¦‚:

```shell
$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item
```


[Spiders Contractså®˜ç½‘](https://docs.scrapy.org/en/latest/topics/contracts.html?highlight=Contracts)

## å†…ç½®åˆçº¦é¡¹

#### @url

- `@url` : è®¾ç½®æ£€æŸ¥åˆçº¦æ—¶çš„sample urlï¼Œå¦‚æœæ²¡æœ‰å®ƒï¼Œåˆ™åœ¨`scrapy check ...`æ—¶å¿½ç•¥è¯¥è¯¥åˆçº¦ã€‚è¯­æ³•æ˜¯ï¼š`@url url`ã€‚å¯¹åº”ç±»ä¸º`classscrapy.contracts.default.UrlContract`

#### @returns

- `@returns`: è®¾ç½®æ£€æŸ¥åˆçº¦æ—¶spdieråº”è¿”å›çš„`items and requests`ä¸ªæ•°ä¸‹é™ä¸ä¸Šé™, è¯­æ³•æ˜¯`@returns item(s)|request(s) [min [max]]`. å¯¹åº”ç±»ä¸º`classscrapy.contracts.default.ReturnsContract[source]`

#### @cb_kwargs

- `@cb_kwargs` : è®¾ç½®æ£€æŸ¥åˆçº¦æ—¶`sample request`çš„`cb_kwargs attribute `. ä¾‹å¦‚`"@cb_kwargs {"arg1": "value1", "arg2": "value2", ...}"` .å¯¹åº”ç±»ä¸º `classscrapy.contracts.default.CallbackKeywordArgumentsContract[source]`

#### @scrapes
- `@scrapes` : æ£€æŸ¥æ£€æŸ¥åˆçº¦æ—¶callbackè¿”å›çš„`items`åº”æ»¡è¶³çš„æˆå‘˜å­—æ®µåˆ—è¡¨ï¼Œä¾‹å¦‚`@scrapes field_1 field_2 ...` . å¯¹åº”ç±»ä¸º`classscrapy.contracts.default.ScrapesContract[source]`

## å®šåˆ¶åˆçº¦é¡¹

### åˆ›å»º

é€šè¿‡ç»§æ‰¿`scrapy.contracts.Contract`å¯ä»¥åˆ›å»ºè‡ªå®šä¹‰åˆçº¦

```python
from scrapy.contracts import Contract
from scrapy.exceptions import ContractFail

class HasHeaderContract(Contract):
    """ Demo contract which checks the presence of a custom header
        @has_header X-CustomHeader
    """

    name = 'has_header'

    def pre_process(self, response):
        for header in self.args:
            if header not in response.headers:
                #é€šè¿‡è¿”å›ContractFailï¼ŒæŒ‡ç¤ºä¸æ»¡è¶³åˆçº¦
                raise ContractFail('X-CustomHeader not present')
```

### åŠ è½½

é€šè¿‡é…ç½®æ–‡ä»¶ä¸­çš„`SPIDER_CONTRACTS` åŠ è½½ï¼Œ ä¾‹å¦‚

```python
SPIDER_CONTRACTS = {
    'myproject.contracts.ResponseCheck': 10,
    'myproject.contracts.ItemValidate': 10,
}
```


# å·¥å…·ç±»

### urlparse.urljoin

ä½œç”¨ï¼šç›¸å¯¹urlè½¬æ¢ä¸ºç»å¯¹url

```python
>>>  from urllib.parse import urljoin

>>> help(urljoin)
Help on function urljoin in module urlparse:
 
urljoin(base, url, allow_fragments=True)
    Join a base URL and a possibly relative URL to form an absolute
    interpretation of the latter.

## ä¾‹å­
In [2]: from urllib.parse import urljoin

In [3]: urljoin("http://domain/folder/currentpage.html", "anotherpage.html")
Out[3]: 'http://domain/folder/anotherpage.html'

In [4]: urljoin("http://domain/folder/xyz/currentpage.html", "/anotherpage.html")
Out[4]: 'http://domain/anotherpage.html'

In [5]: urljoin("http://domain/folder/currentpage.html", "folder2/anotherpage.html")
Out[5]: 'http://domain/folder/folder2/anotherpage.html'

In [6]: urljoin("http://domain/folder/currentpage.html", "/folder2/anotherpage.html")
Out[6]: 'http://domain/folder2/anotherpage.html'

In [7]: urljoin("http://domain/abc/folder/currentpage.html", "/folder2/anotherpage.html")
Out[7]: 'http://domain/folder2/anotherpage.html'

In [8]: urljoin("http://domain/abc/folder/currentpage.html", "../anotherpage.html")
Out[8]: 'http://domain/abc/anotherpage.html'

In [9]:  
```
### ç¼–è§£ç è§£ç JSä¸­çš„encodeURIComponentä¸decodeURIComponent

å¼•ç”¨urllibåŒ…ä¸­parseæ¨¡å—çš„quoteå’Œunquote

JSï¼š

```js
encodeURIComponent(â€˜æˆ‘çˆ±ç¼–ç¨‹â€™)
```

Pythonï¼š

```python
from urllib import parse
jsStr=â€™%E6%88%91%E7%88%B1%E7%BC%96%E7%A8%8Bâ€™
print(parse.unquote(jsStr)) 
#â€”> è¾“å‡ºï¼šæˆ‘çˆ±ç¼–ç¨‹
print(jsStr==parse.quote(â€˜æˆ‘çˆ±ç¼–ç¨‹â€™)) 
#â€”>è¾“å‡ºï¼šTrue
```

### scrapy.http.FormRequest

è¯¥ç±»å…¸å‹çš„ä½¿ç”¨åœºæ™¯æ˜¯å‘é€ç™»å½•çš„userã€passwdã€‚ ä¾‹å¦‚ï¼š

```python
# Start on the welcome page
def start_requests(self):
    return [
        Request(
        "http://web:9312/dynamic/nonce",
        callback=self.parse_welcome)
    ]
# Post welcome page's first form with the given user/pass
def parse_welcome(self, response):
    return FormRequest.from_response(
        response,
        formdata={"user": "user", "pass": "pass"}
        )
```

### ç¼–ç é—®é¢˜

å¦‚æœå­˜åœ¨ç¼–ç é—®é¢˜ï¼Œå¯ä»¥å°è¯•ï¼š

```python
unicode(response.body.decode(response.encoding)).encode('utf-8')
newresponse = response.replace(encoding='utf-8')
```

# ååçˆ¬

#### åŠ¨æ€å˜åŒ–userAgent

##### å¢åŠ é…ç½®

åœ¨é¡¹ç›®çš„â€settings.pyâ€œä¸­å¢åŠ userAgentå¤„ç†çš„é…ç½®
```python
USER_AGENT_LIST = [...]
# Enable or disable downloader middlewares
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
DOWNLOADER_MIDDLEWARES = {
   'my.middlewares.RandomUserAgentMiddleware': 543,
#    'my.middlewares.MyDownloaderMiddleware': 543,
}
```

##### å¢åŠ RandomUserAgentMiddleware

åœ¨é¡¹ç›®çš„â€œmiddlewares.pyâ€ä¸­å¢åŠ `RandomUserAgentMiddleware`

```python
# éšæœºé€‰æ‹© User-Agent çš„ä¸‹è½½å™¨ä¸­é—´ä»¶
class RandomUserAgentMiddleware(object):
    def process_request(self, request, spider):
        # ä» settings çš„ USER_AGENTS åˆ—è¡¨ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªä½œä¸º User-Agent
        user_agent = random.choice(spider.settings['USER_AGENT_LIST'])
        request.headers['User-Agent'] = user_agent
        return None
 
    def process_response(self, request, response, spider):
        # éªŒè¯ User-Agent è®¾ç½®æ˜¯å¦ç”Ÿæ•ˆ
        logger.info("headers ::> User-Agent = " + str(request.headers['User-Agent'], encoding="utf8"))
        return response
```

##### å¢åŠ ä¸‹è½½é—´éš”

```python
# Configure a delay for requests for the same website (default: 0)
# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
DOWNLOAD_DELAY = 3
```

#### åŠ¨æ€IPtodo

[Pythonçˆ¬è™«ä»£ç†IPæ± (proxy pool)](https://github.com/jhao104/proxy_pool)

# scrapyé…ç½®å‚æ•°

ç³»ç»Ÿé»˜è®¤é…ç½®é¡¹åˆ—è¡¨ï¼Œå¯ä»¥åœ¨"scrapy\settings\default_settings.py"ä¸­æŸ¥çœ‹åˆ°ï¼Œä¹Ÿå¯ä»¥æŸ¥çœ‹é¡¹ç›®ä¸­çš„"scrapy.cfg"ï¼Œä½†ä¸€èˆ¬è¿™é‡Œé¢æ¯”è¾ƒå°‘ã€‚

é‡è¦çš„é…ç½®å‚æ•°å¦‚ä¸‹ï¼š

![é‡è¦çš„é…ç½®å‚æ•°](#images/EssentialSettings.png)



### logging for analysis


#### LOG_LEVEL
  æ—¥å¿—å±‚çº§åŒ…æ‹¬ï¼šDEBUG (lowest level), INFO, WARNING, ERROR, and CRITICAL (highest level). ä»¥åŠSILENT ï¼ˆä¸æ‰“å°æ—¥å¿—ï¼‰
#### LOGSTATS_INTERVAL
  æ‰“å°æ—¥å¿—çš„é¢‘åº¦ï¼Œé»˜è®¤60s
#### LOG_ENABLED
#### LOG_FILE
  é»˜è®¤æ˜¯stdout  
#### LOG_STDOUT
  å¦‚æœä¸ºtrueï¼Œæ„å‘³ç€æ‰€æœ‰è¾“å‡ºåˆ°stdoutçš„éƒ½æ‰“å°åˆ°logä¸­

### stats for analysis

#### STATS_DUMP
  é»˜è®¤æ˜¯å¼€å¯çš„
#### DOWNLOADER_STATS
  è®¾ç½®ä¸ºfalseï¼Œä»£è¡¨å¯¹downloadä¸å‰åˆ©stats
#### DEPTH_STATS
  è®¾ç½®æ˜¯å¦ç»Ÿè®¡site depth
#### DEPTH_STATS_VERBOSE
  å¦‚æœéœ€è¦site depthæ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œè®¾ç½®å®ƒ
#### STATSMAILER_RECPTS
  å½“crawl finishæ—¶å‘é€statsåˆ°å“ªä¸ªé‚®ç®±

### telent  for analysis

#### TELENTCONSOLE_ENABLED
#### TELENTCONSOLE_PORT

### feeds

#### FEED_URI
#### FEED_PORMAT
#### FEED_STORE_EMPTY
#### FEED_EXPORT_FIELDS
#### FEED_URI_PARAMS

### crawling style

#### DEPTH_LIMIT
#### DEPTH_PRIORITY
#### SCHEDULER_DISK_QUEUE
#### SCHEDULER_MEMOY_QUEUE
#### ROBOTSTXT_OBKEY
#### COOKIES_ENABLED
#### REFERER_ENABLED
#### USER_AGENT
#### DEFAULT_REQUEST_HEADERS


### performance

#### CONCURRENT_REQUESTS

#### CONCURRENT_REQUESTS_PER_DOMAIN

#### CONCURRENT_REQUESTS_PER_IP

#### CONCURRENT_ITEMS

#### DOWNLOAD_TIMEOUT

#### DOWNLOAD_DELAY

#### RANDOMIZE_DOWNLOAD_DELAY

#### DNSCACHE_ENABLED

### closing

#### CLOSESPIDER_ERRORCOUNT

#### CLOSESPIDER_ITEMCOUNT

#### CLOSESPIDER_PAGECOUNT

#### CLOSESPIDER_TIMEOUT

### http cache

#### HTTPCACHE_ENABLED

#### HTTPCACHE_DIR

#### HTTPCACHE_POLICY

#### HTTPCACHE_STORAGE

#### HTTPCACHE_DBM_MODULE

#### HTTPCACHE_EXPIRATION_SECS

#### HTTPCACHE_IGNORE_HTTP_CODES
#### HTTPCACHE_IGNORE_SCHEMES
#### HTTPCACHE_GZIP

### media download

#### IMAGES_STORE
#### IMAGES_EXPIRES
#### IMAGES_THUMBS
#### IMAGES_URLS_FIELD
#### IMAGES_RESULT_FIELD
#### IMAGES_MIN_HEIGHT
#### IMAGES_MIN_WIDTH
#### FILES_STORE
#### FILES_EXPIRES
#### FILES_URLS_FIELD
#### FILES_RESULT_FIELD

# gpai æŠ“å–è®°å½•

### é“¾æ¥é¢„åˆ†æ

gpaiç½‘æ˜¯åŠ¨æ€ç½‘é¡µï¼Œä½¿ç”¨é™æ€æŠ“å–ä¸å¯è¡Œï¼Œå› æ­¤é‡‡ç”¨selenium+scrapçš„æ–¹å¼
#### é¢„åˆ†æç»“æœ

ä¸‹é¢çš„ä»£ç æ˜¯æ‰‹å·¥åˆ†ææ‰§è¡Œçš„ç»“æœ

```python
# æ”¯æŒIn : webdriver.Chrome  webdriver.Firefox  webdriver.Edge webdriver.FirefoxOptions ã€‚ã€‚ã€‚
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
#åˆ›å»ºå®ä¾‹ï¼Œç›´åˆ°é¡µé¢oloadæ–¹æ³•æ‰§è¡Œå®Œæ¯•æ‰è¿”å›
driver = webdriver.Firefox()
driver.get("http://s.gpai.net/sf/search.do?action=court&cityNum=310109&Page=1")
#ç­‰å¾…åˆ†é¡µå¯¼èˆªå‡ºç°
sel = 'body .page-bar'
try:
    WebDriverWait(driver, 10).until(
        lambda driver: driver.find_element_by_css_selector(sel))
except TimeoutException:
    print('åŠ è½½å¤±è´¥')
    pass
from scrapy.selector import Selector
body=Selector(text=driver.page_source)
#è·å–åˆ†é¡µé“¾æ¥list
pages = body.css(f'{sel}'+' ' +'a::attr("href")').getall()
#ç»“æœç±»ä¼¼ä¸‹é¢ï¼Œéœ€è¦ä»ä¸­åˆ†æmax page
# ['http://s.gpai.net/sf/search.do?action=court&cityNum=310109&Page=0',
#  'http://s.gpai.net/sf/search.do?action=court&cityNum=310109&Page=1',
#...
#  'http://s.gpai.net/sf/search.do?action=court&cityNum=310109&Page=16',
#  'http://s.gpai.net/sf/search.do?action=court&cityNum=310109&Page=2']

#è·å–é¡µé¢æ¯é¡¹èµ„æºlink
sel = '.main-col-list .list-item .item-tit a::attr("href")'
items = body.css(sel).getall()
##ç»“æœç±»ä¼¼ä¸‹é¢
# ['http://www.gpai.net/sf/item2.do?Web_Item_ID=28120',
# ...
#  'http://www.gpai.net/sf/item2.do?Web_Item_ID=28562']

#
driver = webdriver.Firefox()
driver.get('http://www.gpai.net/sf/item2.do?Web_Item_ID=28562')
body=Selector(text=driver.page_source)
sel = '.tab-list-con .panel-con'
# xx = body.css(sel).getall()
##è·å–ç«ä¹°é¡»çŸ¥
sel = '#menuid2 + .d-title + .d-article'
item_notice = body.css(sel).get()
##è·å–æ ‡çš„ç‰©ä»‹ç»
sel = '#menuid3 + .d-title + .d-block'
item_intro = body.css(sel).get()
##è·å–æ ‡çš„é™„ä»¶
sel = '.d-block-download a'
sels = body.css(sel)
item_attachs=dict()
for i in sels:
    title = i.css('a::text').get()
    href = i.css('a::attr("href")').get()
    item_attachs[title]=href
##è·å–æ ‡çš„è§†é¢‘ä»‹ç»
sel='.d-block > .d-article video'
item_video=dict()
try:
    item_video['src']= body.css(sel+'::attr(src)').get().strip()
    item_video['poster']= body.css(sel+'::attr(poster)').get().strip()
except Exception:
    pass
##è·å–å›¾ç‰‡
sel='.ItemImgAll img'
item_images:list=body.css(sel+'::attr(src)').getall()
##è·å–ä¼˜å…ˆè´­ä¹°äººä¿¡æ¯
sel='.d-block .d-block-tb table'
item_preferred_buyers=body.css(sel).get()
```



# cchereæŠ“å–è®°å½•

### articleé“¾æ¥é¢„åˆ†æ

articleé“¾æ¥æ˜¯æŒ‡urlä¸ºç±»ä¼¼`https://www.talkcc.net/article/4520813`çš„æ–‡ç« é“¾æ¥

#### æŸ¥çœ‹æŠ“å–åˆ°html source
```shell
# æŸ¥çœ‹html sourceï¼Œé€šè¿‡ä¸‹é¢å‘½ä»¤èƒ½ä½¿å¾—ä¸spiderçœ‹åˆ°çš„ä¸€è‡´
$scrapy view https://www.talkcc.net/article/4520813
```

#### é¢„åˆ†æç»“æœ

å¯¹â€œarticle/xxxâ€é€šè¿‡åˆ†ææœ‰ä»¥ä¸‹å‡ ç‚¹è®¤è¯†ï¼š

- æ–‡ç« æ­£æ–‡å†…å®¹åœ¨â€œ`var ls=`â€å®šä¹‰çš„å˜é‡ä¸­ï¼Œ
- è¯¥å˜é‡éœ€è¦é€šè¿‡ä½äº[`js0.8.3.js`](https://www.talkcc.net/incs/js0.8.3.js)ä¸­çš„`zJ_PE`å‡½æ•°è§£ç æ‰å¯ä»¥å¾—åˆ°æ–‡ç« htmlç‰‡æ®µ
- è¯¥æ–‡ç« htmlç‰‡æ®µä¸­ï¼Œæ­£æ–‡å†…å®¹åœ¨`<article class="sClear">`ä¸­
- 
- æ–‡ç« ä¸­å‡ºç°çš„â€œ`var rs=`â€å˜é‡å­˜å‚¨å†…å®¹ï¼Œæ˜¯æ­£æ–‡å³ä¾§å¾—èŠ±è¯„è®ºåˆ—è¡¨ï¼Œåº”ä¸æ˜¯æˆ‘ä»¬å…³ç³»çš„å†…å®¹

å¯¹`zJ_PE`å‡½æ•°è½¬æ¢ä¸ºpythonå®ç°æ˜¯ï¼š

```python
# function zJ_PE(t) {
# 	var e = t.substring(0, 3);
# 	if ("*" != e.charAt(1) || "@" != e.charAt(2))
# 		return t;
# 	if (t = t.substring(3), 0 < parseInt(e.charAt(0))) {
# 		for (
# 			var n = new Array(/@/g, /#/g, /\$/g, /\?/g, /\^/g, /\[/g, /=/g, /</g, />/g, /\{/g, /\}/g, /\|/g, /:/g, /;/g, /,/g),
# 			i = new Array("%2", "%3", "%4", "%5", "%6", "%7", "%8", "%9", "%A", "%B", "%C", "%D", "%E", "%F", "e"),
# 			a = 0; a < n.length; a++)
# 			t = t.replace(n[a], i[a]);
# 		return decodeURIComponent(t)
# 	} return t
# }

from urllib import parse
#å®ç°js0.8.3.jsä¸­çš„zJ_PEå‡½æ•°pythonç‰ˆæœ¬
def zJ_PE(t:str):
	e=t[:3]
	if '*' != e[1] or '@' != e[2]:
		return t

	#å®šä¹‰è½¬æ¢å­—å…¸
	conv = dict(
		[("@",'%2'),('#','%3'),('$','%4'),('?','%5'),('^','%6'),('[','%7'),('=','%8'),('<','%9'),
		('>','%A'),('{','%B'),('}','%C'),('|','%D'),(':','%E'),(';','%F'),(',','e')]
	)
	
	t=t[3:]
    
	if 0< int(e[0]):
		for k,v in conv.items():
			t=t.replace(str(k),str(v),-1)
		return parse.unquote(t)

	return t
```

### threadé“¾æ¥é¢„åˆ†æ

articleé“¾æ¥æ˜¯æŒ‡urlä¸ºç±»ä¼¼`https://www.talkcc.net/thread/4519373`çš„æ–‡ç« é“¾æ¥

#### æŸ¥çœ‹æŠ“å–åˆ°html source
```shell
# æŸ¥çœ‹html sourceï¼Œé€šè¿‡ä¸‹é¢å‘½ä»¤èƒ½ä½¿å¾—ä¸spiderçœ‹åˆ°çš„ä¸€è‡´
$scrapy view https://www.talkcc.net/thread/4519373
```

#### é¢„åˆ†æç»“æœ

å¯¹â€œthread/xxxâ€é€šè¿‡åˆ†ææœ‰ä»¥ä¸‹å‡ ç‚¹è®¤è¯†ï¼š

- æ­£æ–‡åœ¨â€œ`var ls=`â€å®šä¹‰çš„å˜é‡ä¸­
- è¯¥å˜é‡éœ€è¦é€šè¿‡ä½äº[`js0.8.3.js`](https://www.talkcc.net/incs/js0.8.3.js)ä¸­çš„`zJ_PE`å‡½æ•°è§£ç æ‰å¯ä»¥å¾—åˆ°æ­£æ–‡htmlç‰‡æ®µ
- æ­£æ–‡htmlç‰‡æ®µä¸­ï¼Œæœ€å‰é¢çš„`<h3 class="sTAC sNoWrap">`å«æœ‰æ–‡ç« æ‰€å±ä¸“é¢˜ä¿¡æ¯

  ```python
  <h3 class="sTAC sNoWrap">
      <small>ä¸»é¢˜:</small>å†œä¸šæ–‡æ˜çš„å‡ ä¸ªé—®é¢˜ -- ziyun2015</h3>

  ```

- æ­£æ–‡htmlç‰‡æ®µä¸­ï¼Œ`<div id="D_GT_L" class="s_Sec">`å«æœ‰æ­£æ–‡articls

  ```html
  <div id="D_GT_L" class="s_Sec">
      ...
      <div class="s_SecC" style="margin-left:6em;">
          <article class="sClear">
              <div class="sPSInfo s_Smaller">4525096 å¤ <a href="/article/4525025">4525025</a><br><a href="/user/æ¡¥ä¸Š"
                      c-u=24173>æ¡¥ä¸Š</a></div>
              <div id="PD4525096" class="sPACont" c-mnu="4525096,1,0,0,0,0" c-pzt="4519373"><small>7</small><img
                      src="/bbsIMG/bf/face1.gif"> <b>ä¸å®œç”¨éŸ©éå­å’Œè¯´æ–‡æ¥è§£ï¼Œ</b> <i class="sUpV">1</i> <img src="/bbsIMG/np.gif"
                      alt="æ–°"><br>
                  <p>å› ä¸ºé‚£ä¸ªè§£é‡Šæ˜¯åœ¨ä¹‹å‰ã€‚</p> <br>
                  <ul>
                      <li>æœ¬å¸– <a href="javascript:void(0)" onclick="zJ_DirectGT(this,4525096)">è·Ÿ 1ï¼ŒğŸŒº 4</a></li>
                  </ul>
                  <div class="sHFixedA"></div>
                  <div class="sPMc"><small>2020-06-04 00:39:37</small></div>
                  <div class="sForPER"></div>
              </div>
          </article>
      </div>
      ...
  </div>
  ```

- åœ¨åˆ†é¡µæƒ…å†µä¸‹ï¼Œ é¦–é¡µç­‰äºè¯¥starturl,ç¿»é¡µä¿¡æ¯ä½äºä¸‹é¢çš„ç‰‡æ®µ

  ```html
  <h4 class="s_PGNav s_FW">
      <div class="s_FW_L">å…¨çœ‹ <a href="/topic/4519373">åˆ†é¡µ</a> <a href="/alist/4519373">æ ‘å±•</a> <a
              href="/ainfo/4519373">ä¸€è§ˆ</a> <a href="/article/4519373">ä¸»é¢˜</a></div>
      <div class="s_FW_R">
          <form method="post" onsubmit="location.assign('/thread/4519373/'+this.p.value);return false"><input name="p"
                  size="2" type="text" value="1" class="sCurPNum"> / 6 <a href="/thread/4519373">é¦–é¡µ</a> ä¸Šé¡µ <a
                  href="/thread/4519373/2">ä¸‹é¡µ</a> <a href="/thread/4519373/6">æœ«é¡µ</a></form>
      </div>
  </h4>
  ```


# ä¸‹è½½å™¨ä¸­é—´ä»¶

[ç³»ç»Ÿé»˜è®¤ä¸­é—´ä»¶ï¼Œä»¥åŠä¼˜å…ˆçº§æ•°å­—](https://docs.scrapy.org/en/latest/topics/settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE)ï¼Œé™¤éæ˜¾ç¤ºç¦ç”¨ï¼Œå¦åˆ™ç³»ç»Ÿé»˜è®¤ä¸­é—´ä»¶ä¼šèµ·ä½œç”¨çš„ã€‚

```python
DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.CustomDownloaderMiddleware': 543,
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
}
```
è¿™ä¸Šé¢ä¾‹å­ä¸­ï¼š

- æ•°å­—çš„å«ä¹‰ï¼šå¯¹ä¸­é—´ä»¶`process_request() `æ–¹æ³•ï¼Œå°†ä»¥é€’å¢çš„ä¸­é—´ä»¶é¡ºåºï¼ˆ100ã€200ã€300ï¼Œ...ï¼‰è¢«æ‰§è¡Œã€‚
`process_response()`æ–¹æ³•ç›¸å

- `None`ä»£è¡¨ç¦ç”¨çš„æ„æ€

ä¸‹è½½ä¸­é—´ä»¶process_requestçš„å‡ ç§è¿”å›ï¼š

- å¦‚æœè¿”å›`None`ï¼Œåˆ™Scrapyå°†ç»§ç»­å¤„ç†æ­¤è¯·æ±‚ï¼Œæ‰§è¡Œæ‰€æœ‰å…¶ä»–ä¸­é—´ä»¶ã€‚

- å¦‚æœå®ƒè¿”å›ä¸€ä¸ªResponseå¯¹è±¡ï¼ŒScrapyå°†å¿½ç•¥ä»»ä½•å…¶ä»–process_request()æˆ–process_exception()æ–¹æ³•ï¼Œæˆ–ç›¸åº”çš„ä¸‹è½½åŠŸèƒ½; å®ƒå°†è¿”å›è¯¥å“åº”ã€‚process_response() æ€»æ˜¯åœ¨æ¯ä¸ªå“åº”ä¸Šè°ƒç”¨å·²å®‰è£…ä¸­é—´ä»¶çš„æ–¹æ³•ã€‚

- å¦‚æœè¿”å›ä¸€ä¸ªRequestå¯¹è±¡ï¼ŒScrapyå°†åœæ­¢è°ƒç”¨process_requestæ–¹æ³•å¹¶é‡æ–°è®¡åˆ’è¿”å›çš„è¯·æ±‚ã€‚ä¸€æ—¦æ‰§è¡Œäº†æ–°è¿”å›çš„è¯·æ±‚ï¼Œå°±ä¼šåœ¨ä¸‹è½½çš„å“åº”ä¸Šè°ƒç”¨é€‚å½“çš„ä¸­é—´ä»¶é“¾ã€‚

- å¦‚æœå¼•å‘IgnoreRequestå¼‚å¸¸ï¼Œåˆ™å°†process_exception()è°ƒç”¨å·²å®‰è£…çš„ä¸‹è½½å™¨ä¸­é—´ä»¶çš„ æ–¹æ³•ã€‚å¦‚æœå®ƒä»¬éƒ½ä¸å¤„ç†è¯¥å¼‚å¸¸ï¼ŒRequest.errbackåˆ™è°ƒç”¨è¯·æ±‚ï¼ˆï¼‰çš„errbackå‡½æ•°ã€‚å¦‚æœæ²¡æœ‰ä»£ç å¤„ç†å¼•å‘çš„å¼‚å¸¸ï¼Œåˆ™å°†å…¶å¿½ç•¥å¹¶ä¸”ä¸è®°å½•ï¼ˆä¸å…¶ä»–å¼‚å¸¸ä¸åŒï¼‰ã€‚



# selenium

å‰æï¼š `pip install selenium`

[selenium pythonä¸­æ–‡æ–‡æ¡£](https://selenium-python-zh.readthedocs.io/en/latest/index.html)

### å¿«é€Ÿå…¥æ‰‹

```python
# æ”¯æŒIn : webdriver.Chrome  webdriver.Firefox  webdriver.Edge webdriver.FirefoxOptions ã€‚ã€‚ã€‚
from selenium import webdriver
#é”®ç›˜æ”¯æŒ
from selenium.webdriver.common.keys import Keys

#åˆ›å»ºå®ä¾‹ï¼Œç›´åˆ°é¡µé¢oloadæ–¹æ³•æ‰§è¡Œå®Œæ¯•æ‰è¿”å›
driver = webdriver.Firefox()
elem = driver.find_element_by_name("q")

#ç‰¹æ®Šçš„æŒ‰é”®,æ¯”å¦‚å›è½¦.å¯ä»¥ä½¿ç”¨Keysç±»æ¥è¾“å…¥ï¼Œkeysç±»ç»§æ‰¿è‡ª selenium.webdriver.common.keys
elem.clear()  # ä¸ºäº†å®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬å…ˆæ¸…é™¤inputè¾“å…¥æ¡†ä¸­çš„ä»»ä½•é¢„å¡«å……çš„æ–‡æœ¬
elem.send_keys("pycon") #é”®ç›˜è¾“å…¥
elem.send_keys(Keys.RETURN) #é”®ç›˜è¾“å…¥å›è½¦,
driver.close()  #å…³é—­æ ‡ç­¾é¡µï¼Œå¦‚æœè¦å…³é—­æ•´ä¸ªæµè§ˆå™¨ï¼Œä½¿ç”¨driver.quit()
```

## 

### æ‰“å¼€é¡µé¢

è°ƒç”¨`driver.get("http://www.google.com")` WebDriver å°†ç­‰å¾…ï¼Œç›´åˆ°é¡µé¢å®Œå…¨åŠ è½½å®Œæ¯•ï¼ˆå…¶å®æ˜¯ç­‰åˆ° onload æ–¹æ³•æ‰§è¡Œå®Œæ¯•ï¼‰ï¼Œ ç„¶åè¿”å›ç»§ç»­æ‰§è¡Œä½ çš„è„šæœ¬ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœä½ çš„é¡µé¢ä½¿ç”¨äº†å¤§é‡çš„AjaxåŠ è½½ï¼Œ WebDriverå¯èƒ½ä¸çŸ¥é“ä»€ä¹ˆæ—¶å€™é¡µé¢å·²ç»å®Œå…¨åŠ è½½ã€‚ å¦‚æœä½ æƒ³ç¡®ä¿ä¹Ÿmainå®Œå…¨åŠ è½½å®Œæ¯•ï¼Œå¯ä»¥ä½¿ç”¨:`ref:waits <waits>`

ä½¿ç”¨é€‰æ‹©å™¨`driver.find_element_by_xxx`ï¼Œï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°å°†ä¼šæŠ›å‡º ``NoSuchElementException``å¼‚å¸¸ã€‚

### å¡«å†™è¡¨æ ¼

ä¾‹å¦‚å¯¹ï¼š

```html
<select>
  <option value ="volvo">Volvo</option>
  <option value ="saab">Saab</option>
  <option value="opel">Opel</option>
  <option value="audi">Audi</option>
</select>
```
ä½¿ç”¨selenium Selectç±»
```python
from selenium.webdriver.support.ui import Select
select = Select(driver.find_element_by_name('name'))
select.select_by_index(index)
select.select_by_visible_text("text")
select.select_by_value(value)
```

### ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ(Waits)

æä¾›ä¸¤ç§ç±»å‹çš„waits - éšå¼å’Œæ˜¾å¼ã€‚ æ˜¾å¼ç­‰å¾…ä¼šè®©WebDriverç­‰å¾…æ»¡è¶³ä¸€å®šçš„æ¡ä»¶ä»¥åå†è¿›ä¸€æ­¥çš„æ‰§è¡Œã€‚ è€Œéšå¼ç­‰å¾…è®©Webdriverç­‰å¾…ä¸€å®šçš„æ—¶é—´åå†æ‰æ˜¯æŸ¥æ‰¾æŸå…ƒç´ ã€‚

#### æ˜¾å¼ç­‰å¾…ä¾‹å­

- ä¾‹1
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

driver = webdriver.Firefox()
driver.get("http://somedomain/url_that_delays_loading")
try:
    element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.ID, "myDynamicElement"))
    )
finally:
    driver.quit()
```
- ä¾‹2
```python
WebDriverWait(driver, 100).until(
            lambda driver: driver.find_element_by_name(self.locator))
```

#### éšå¼ç­‰å¾…
å¦‚æœæŸäº›å…ƒç´ ä¸æ˜¯ç«‹å³å¯ç”¨çš„ï¼Œéšå¼ç­‰å¾…æ˜¯å‘Šè¯‰WebDriverå»ç­‰å¾…ä¸€å®šçš„æ—¶é—´åå»æŸ¥æ‰¾å…ƒç´ ã€‚ é»˜è®¤ç­‰å¾…æ—¶é—´æ˜¯0ç§’ï¼Œä¸€æ—¦è®¾ç½®è¯¥å€¼ï¼Œéšå¼ç­‰å¾…æ˜¯è®¾ç½®è¯¥WebDriverçš„å®ä¾‹çš„ç”Ÿå‘½å‘¨æœŸã€‚

```python
from selenium import webdriver

driver = webdriver.Firefox()
driver.implicitly_wait(10) # seconds
driver.get("http://somedomain/url_that_delays_loading")
myDynamicElement = driver.find_element_by_id("myDynamicElement")
```

#### å¤šä¸ªå…ƒç´ çš„ç­‰å¾…

https://stackoverflow.com/questions/22710154/python-selenium-implicit-wait-for-multiple-elements
```python
WebDriverWait(browser, 10).until(
    EC.presence_of_all_elements_located((By.CSS_SELECTOR, '#page a'))
)
```

### API

#### Action Chains
```python
from selenium import webdriver
webdriver.ActionChains
menu = driver.find_element_by_css_selector(".nav")
hidden_submenu = driver.find_element_by_css_selector(".nav #submenu1")

ActionChains(driver).move_to_element(menu).click(hidden_submenu).perform()
```
#### é€‰æ‹©å™¨

é€šè¿‡`By`ï¼š
```python
from selenium.webdriver.common.by import By

driver.find_element(By.XPATH, '//button[text()="Some text"]')
driver.find_elements(By.XPATH, '//button')
```

é€šè¿‡driver.`find_elements_by_xxx`



### å¸¸è§é—®é¢˜

æ›´å¤šè§[Frequently Asked Questions](https://github.com/SeleniumHQ/selenium/wiki/Frequently-Asked-Questions)

#### 403é—®é¢˜TODO

https://stackoverflow.com/questions/33225947/can-a-website-detect-when-you-are-using-selenium-with-chromedriver

#### æŸ¥çœ‹selienumçš„user-agent

How to get user agent information in Selenium WebDriver with Python
é€šè¿‡æ‰§è¡Œjs
`user_agent = driver.execute_script("return navigator.userAgent;")`

#### æ‰§è¡Œjs

```python
driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
```

å¦‚æœæ˜¯javaï¼Œåˆ™ç±»ä¼¼:

```java
WebDriver driver; // Assigned elsewhere
JavascriptExecutor js = (JavascriptExecutor) driver;
js.executeScript("return document.title");
```

æ‰§è¡Œjséœ€è¦è¿”å›å€¼ï¼Œå–å†³äºjsä»£ç ä¸­æ˜¯å¦æœ‰è¿”å›

`js.executeScript("document.title");`will return null, but:

`js.executeScript("return document.title");`will return the title of the document.

#### å¦‚æœè·å–å½“å‰çª—å£çš„æˆªå›¾ï¼Ÿ
ä½¿ç”¨webdriveræä¾›çš„ save_screenshot æ–¹æ³•:

```python
from selenium import webdriver

driver = webdriver.Firefox()
driver.get('http://www.python.org/')
driver.save_screenshot('screenshot.png')
driver.quit()
```

[Selenium with Pythonä¸­æ–‡ç¿»è¯‘æ–‡æ¡£](https://selenium-python-zh.readthedocs.io/en/latest/index.html)

[WebDriver for Firefox ä¸‹è½½è·¯å¾„](https://firefox-source-docs.mozilla.org/testing/geckodriver/), [related github](https://github.com/mozilla/geckodriver),éœ€è¦å°†geckodriveræ”¾ç½®åœ¨PATHå¯æ‰§è¡Œè·¯å¾„ä¸­

[WebDriver for Chrome ä¸‹è½½è·¯å¾„ ](https://sites.google.com/a/chromium.org/chromedriver/downloads)  é€šè¿‡`chrome://version/`æŸ¥æ‰¾chromeå®‰è£…è·¯å¾„ä¸chromeç‰ˆæœ¬.éœ€è¦å°†`chromedriver.exe`æ”¾ç½®åœ¨PATHå¯æ‰§è¡Œè·¯å¾„ä¸­

        time.sleep(4)
        # é€šè¿‡æ–­è¨€é¡µé¢æ˜¯å¦å­˜åœ¨æŸäº›å…³é”®å­—æ¥ç¡®å®šé¡µé¢æŒ‰ç…§é¢„æœŸåŠ è½½
        assert u"é¦–é¡µ--å…‰è£ä¹‹è·¯" in self.driver.page_source, u"é¡µé¢æºç ä¸­ä¸å­˜åœ¨è¯¥å…³é”®å­—ï¼"

[selenium webdriveræ€æ ·åˆ¤æ–­ç½‘é¡µåŠ è½½å®Œ](https://jingyan.baidu.com/article/2d5afd69bfaaffc4a3e28e54.html)

ä¸€ä¸ªç®€å•çš„æ ·ä¾‹

```python
import scrapy
from selenium import webdriver

class ProductSpider(scrapy.Spider):
    name = "product_spider"
    allowed_domains = ['ebay.com']
    start_urls = [...']

    def __init__(self):
        self.driver = webdriver.Firefox()

    def parse(self, response):
        self.driver.get(response.url)

        while True:
            next = self.driver.find_element_by_xpath('//td[@class="pagn-next"]/a')

            try:
                next.click()

                # get the data and write it to scrapy items
            except:
                break

        self.driver.close()
```
